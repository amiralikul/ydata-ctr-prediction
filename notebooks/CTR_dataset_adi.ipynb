{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I6lxoD40XD7V"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import duckdb\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7KffdsVt4gW",
    "outputId": "1efcb122-e99d-4693-bce7-e76dade5a6ab"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('train_dataset_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "TFC7ITHhvBYm"
   },
   "outputs": [],
   "source": [
    "#impute\n",
    "def fill_gender_age_based_on_user_group(row):\n",
    "    if ((pd.isna(row['gender'])) and (pd.notna(row['user_group_id']))):\n",
    "      row['gender'] = 'Male' if row['user_group_id'] <= 6 else 'Female'\n",
    "    if ((pd.isna(row['age_level'])) and (pd.notna(row['user_group_id']))):\n",
    "      row['age_level'] = row['user_group_id'] if row['user_group_id'] <= 6 else row['user_group_id'] - 6\n",
    "    if ((pd.isna(row['user_group_id'])) and (pd.notna(row['age_level'])) and (pd.notna(row['gender']))):\n",
    "        row['user_group_id'] = row['age_level'] if row['gender'] == 'Male' else row['age_level'] + 6\n",
    "    return row\n",
    "\n",
    "def impute(df):\n",
    "  df.dropna(how='all',inplace=True)\n",
    "  df.dropna(subset=['is_click'], inplace=True)\n",
    "\n",
    "  df[df['webpage_id'] != 13787].webpage_id[df['campaign_id'].notna()] = df.groupby('campaign_id')['webpage_id'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "  df.campaign_id[df['webpage_id'].notna()] = df.groupby('webpage_id')['campaign_id'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
    "\n",
    "  col_lists = ['gender','age_level','user_depth','user_group_id','city_development_index']\n",
    "  for col in col_lists:\n",
    "    subset_filled = df.groupby('user_id')[col].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)\n",
    "    df[col] = df[col].fillna(df['user_id'].map(subset_filled))\n",
    "\n",
    "  df = df.apply(fill_gender_age_based_on_user_group, axis=1)\n",
    "\n",
    "  df.drop_duplicates(inplace=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns that must be without nulls before create new variables\n",
    "def columns_cant_be_with_nulls(df,cols):\n",
    "    for col in cols:\n",
    "        df.dropna(subset=[col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "6a04QkH2v2Xd"
   },
   "outputs": [],
   "source": [
    "def create_new_variables(df):\n",
    "  df.DateTime = pd.to_datetime(df.DateTime)\n",
    "  df['hour'] = df.DateTime.dt.hour\n",
    "  df['time_of_day'] = pd.cut(df['hour'],bins=[-np.inf, 6, 12, 18, np.inf],labels=['night', 'morning', 'afternoon', 'evening'])\n",
    "  df['day_of_the_week'] = df.DateTime.dt.day_name()\n",
    "  day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "  df['day_of_the_week'] = pd.Categorical(df['day_of_the_week'], categories=day_order, ordered=True)\n",
    "\n",
    "\n",
    "  df = df.sort_values(by=['user_id', 'DateTime'])\n",
    "  df['exposures_so_far'] = df.groupby('user_id').cumcount()\n",
    "  df['first_exposure'] = df['exposures_so_far'].apply(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "  df['clicks_so_far'] = df.groupby('user_id')['is_click'].cumsum()\n",
    "\n",
    "  df['clicked_before'] = df['clicks_so_far'].apply(lambda x: 0 if x == 0 else 1)\n",
    "\n",
    "  df['clicks_divided_exposures'] = np.where(df['exposures_so_far'] == 0, 0, df['clicks_so_far'] / df['exposures_so_far'])\n",
    "  return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to drop - session_id, user_id, product_category_2, datetime\n",
    "def drop_columns(df,cols):\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UzuDL3FSxGXK"
   },
   "outputs": [],
   "source": [
    "def defualt_nan(df):\n",
    "  defaults = {\"gender\": df[\"gender\"].mode().iloc[0],\n",
    "              \"age_level\": df[\"age_level\"].mode().iloc[0],\n",
    "              \"user_depth\": df[\"user_depth\"].mode().iloc[0],\n",
    "              #\"user_group_id\": df[\"user_group_id\"].mode().iloc[0],\n",
    "              \"var_1\": df[\"var_1\"].mode().iloc[0]}\n",
    "    \n",
    "  for column, default in defaults.items():\n",
    "      df[column] = df[column].fillna(default)\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "fypMAxuKw7L2"
   },
   "outputs": [],
   "source": [
    "def create_dummies(df,cols):\n",
    "  for col in cols:\n",
    "      dummies = pd.get_dummies(df[col], prefix=col, drop_first=True)\n",
    "      df = pd.concat([df, dummies], axis=1)\n",
    "      df = df.drop(col, axis=1)\n",
    "      for dummy in dummies:\n",
    "          df[dummy] = df[dummy].astype(int)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_742882/2305918933.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[df['webpage_id'] != 13787].webpage_id[df['campaign_id'].notna()] = df.groupby('campaign_id')['webpage_id'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n",
      "/tmp/ipykernel_742882/2305918933.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.campaign_id[df['webpage_id'].notna()] = df.groupby('webpage_id')['campaign_id'].transform(lambda x: x.fillna(method='ffill').fillna(method='bfill'))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_level</th>\n",
       "      <th>user_depth</th>\n",
       "      <th>var_1</th>\n",
       "      <th>is_click</th>\n",
       "      <th>exposures_so_far</th>\n",
       "      <th>first_exposure</th>\n",
       "      <th>clicks_so_far</th>\n",
       "      <th>clicked_before</th>\n",
       "      <th>clicks_divided_exposures</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_the_week_Monday</th>\n",
       "      <th>day_of_the_week_Tuesday</th>\n",
       "      <th>day_of_the_week_Wednesday</th>\n",
       "      <th>day_of_the_week_Thursday</th>\n",
       "      <th>day_of_the_week_Friday</th>\n",
       "      <th>day_of_the_week_Saturday</th>\n",
       "      <th>product_category_1_2.0</th>\n",
       "      <th>product_category_1_3.0</th>\n",
       "      <th>product_category_1_4.0</th>\n",
       "      <th>product_category_1_5.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>189904</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99358</th>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341084</th>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364292</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292983</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age_level  user_depth  var_1  is_click  exposures_so_far  \\\n",
       "189904        5.0         2.0    0.0       0.0                 0   \n",
       "99358         3.0         3.0    1.0       0.0                 0   \n",
       "341084        0.0         3.0    1.0       0.0                 0   \n",
       "364292        2.0         3.0    1.0       0.0                 0   \n",
       "292983        4.0         2.0    1.0       0.0                 0   \n",
       "\n",
       "        first_exposure  clicks_so_far  clicked_before  \\\n",
       "189904               1            0.0               0   \n",
       "99358                1            0.0               0   \n",
       "341084               1            0.0               0   \n",
       "364292               1            0.0               0   \n",
       "292983               1            0.0               0   \n",
       "\n",
       "        clicks_divided_exposures  gender_Male  ...  day_of_the_week_Monday  \\\n",
       "189904                       0.0            0  ...                       0   \n",
       "99358                        0.0            1  ...                       0   \n",
       "341084                       0.0            1  ...                       0   \n",
       "364292                       0.0            1  ...                       1   \n",
       "292983                       0.0            1  ...                       0   \n",
       "\n",
       "        day_of_the_week_Tuesday  day_of_the_week_Wednesday  \\\n",
       "189904                        0                          0   \n",
       "99358                         0                          0   \n",
       "341084                        0                          1   \n",
       "364292                        0                          0   \n",
       "292983                        0                          0   \n",
       "\n",
       "        day_of_the_week_Thursday  day_of_the_week_Friday  \\\n",
       "189904                         0                       0   \n",
       "99358                          1                       0   \n",
       "341084                         0                       0   \n",
       "364292                         0                       0   \n",
       "292983                         1                       0   \n",
       "\n",
       "        day_of_the_week_Saturday  product_category_1_2.0  \\\n",
       "189904                         0                       0   \n",
       "99358                          0                       0   \n",
       "341084                         0                       1   \n",
       "364292                         0                       1   \n",
       "292983                         0                       0   \n",
       "\n",
       "        product_category_1_3.0  product_category_1_4.0  product_category_1_5.0  \n",
       "189904                       0                       0                       0  \n",
       "99358                        1                       0                       0  \n",
       "341084                       0                       0                       0  \n",
       "364292                       0                       0                       0  \n",
       "292983                       0                       0                       1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = impute(df)\n",
    "df = columns_cant_be_with_nulls(df,['user_id','session_id','DateTime'])\n",
    "df = create_new_variables(df)\n",
    "df = drop_columns(df,['session_id','user_id','DateTime','product_category_2','city_development_index','hour','user_group_id'])\n",
    "df = defualt_nan(df)\n",
    "df = create_dummies(df,['gender','product','campaign_id','webpage_id','time_of_day','day_of_the_week','product_category_1'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age_level                    0\n",
       "user_depth                   0\n",
       "var_1                        0\n",
       "is_click                     0\n",
       "exposures_so_far             0\n",
       "first_exposure               0\n",
       "clicks_so_far                0\n",
       "clicked_before               0\n",
       "clicks_divided_exposures     0\n",
       "gender_Male                  0\n",
       "product_B                    0\n",
       "product_C                    0\n",
       "product_D                    0\n",
       "product_E                    0\n",
       "product_F                    0\n",
       "product_G                    0\n",
       "product_H                    0\n",
       "product_I                    0\n",
       "product_J                    0\n",
       "campaign_id_98970.0          0\n",
       "campaign_id_105960.0         0\n",
       "campaign_id_118601.0         0\n",
       "campaign_id_359520.0         0\n",
       "campaign_id_360936.0         0\n",
       "campaign_id_396664.0         0\n",
       "campaign_id_404347.0         0\n",
       "campaign_id_405490.0         0\n",
       "campaign_id_414149.0         0\n",
       "webpage_id_6970.0            0\n",
       "webpage_id_11085.0           0\n",
       "webpage_id_13787.0           0\n",
       "webpage_id_28529.0           0\n",
       "webpage_id_45962.0           0\n",
       "webpage_id_51181.0           0\n",
       "webpage_id_53587.0           0\n",
       "webpage_id_60305.0           0\n",
       "time_of_day_morning          0\n",
       "time_of_day_afternoon        0\n",
       "time_of_day_evening          0\n",
       "day_of_the_week_Monday       0\n",
       "day_of_the_week_Tuesday      0\n",
       "day_of_the_week_Wednesday    0\n",
       "day_of_the_week_Thursday     0\n",
       "day_of_the_week_Friday       0\n",
       "day_of_the_week_Saturday     0\n",
       "product_category_1_2.0       0\n",
       "product_category_1_3.0       0\n",
       "product_category_1_4.0       0\n",
       "product_category_1_5.0       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L7taNkKBFOK5",
    "outputId": "62c4347c-0740-4bf7-f8ec-7177e3ba08bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask\n",
      "  Downloading dask-2025.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting dask-ml\n",
      "  Downloading dask_ml-2024.4.4-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl.metadata (17 kB)\n",
      "Collecting catboost\n",
      "  Downloading catboost-1.2.7-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: click>=8.1 in ./venv/lib/python3.12/site-packages (from dask) (8.1.8)\n",
      "Collecting cloudpickle>=3.0.0 (from dask)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting fsspec>=2021.09.0 (from dask)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from dask) (24.2)\n",
      "Collecting partd>=1.4.0 (from dask)\n",
      "  Downloading partd-1.4.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in ./venv/lib/python3.12/site-packages (from dask) (6.0.2)\n",
      "Collecting toolz>=0.10.0 (from dask)\n",
      "  Using cached toolz-1.0.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting dask-glm>=0.2.0 (from dask-ml)\n",
      "  Downloading dask_glm-0.3.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting distributed>=2.4.0 (from dask-ml)\n",
      "  Downloading distributed-2025.1.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting multipledispatch>=0.4.9 (from dask-ml)\n",
      "  Using cached multipledispatch-1.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting numba>=0.51.0 (from dask-ml)\n",
      "  Downloading numba-0.61.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy>=1.20.0 in ./venv/lib/python3.12/site-packages (from dask-ml) (2.2.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in ./venv/lib/python3.12/site-packages (from dask-ml) (2.2.3)\n",
      "Requirement already satisfied: scikit-learn>=1.2.0 in ./venv/lib/python3.12/site-packages (from dask-ml) (1.6.1)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from dask-ml) (1.15.1)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting tqdm (from umap-learn)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m828.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "Collecting graphviz (from catboost)\n",
      "  Downloading graphviz-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: matplotlib in ./venv/lib/python3.12/site-packages (from catboost) (3.10.0)\n",
      "Collecting numpy>=1.20.0 (from dask-ml)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Collecting plotly (from catboost)\n",
      "  Downloading plotly-6.0.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: six in ./venv/lib/python3.12/site-packages (from catboost) (1.17.0)\n",
      "Collecting sparse>=0.7.0 (from dask-glm>=0.2.0->dask-ml)\n",
      "  Downloading sparse-0.15.5-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting pyarrow>=14.0.1 (from dask[array,dataframe]>=2.4.0->dask-ml)\n",
      "  Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in ./venv/lib/python3.12/site-packages (from distributed>=2.4.0->dask-ml) (3.1.5)\n",
      "Collecting locket>=1.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading locket-1.0.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting msgpack>=1.0.2 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in ./venv/lib/python3.12/site-packages (from distributed>=2.4.0->dask-ml) (6.1.1)\n",
      "Collecting sortedcontainers>=2.0.5 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tblib>=1.6.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading tblib-3.0.0-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: tornado>=6.2.0 in ./venv/lib/python3.12/site-packages (from distributed>=2.4.0->dask-ml) (6.4.2)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in ./venv/lib/python3.12/site-packages (from distributed>=2.4.0->dask-ml) (2.3.0)\n",
      "Collecting zict>=3.0.0 (from distributed>=2.4.0->dask-ml)\n",
      "  Downloading zict-3.0.0-py2.py3-none-any.whl.metadata (899 bytes)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->dask-ml)\n",
      "  Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas>=0.24.2->dask-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.12/site-packages (from pandas>=0.24.2->dask-ml) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./venv/lib/python3.12/site-packages (from pandas>=0.24.2->dask-ml) (2025.1)\n",
      "Requirement already satisfied: joblib>=0.11 in ./venv/lib/python3.12/site-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./venv/lib/python3.12/site-packages (from scikit-learn>=1.2.0->dask-ml) (3.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (4.55.8)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib->catboost) (3.2.1)\n",
      "Collecting narwhals>=1.15.1 (from plotly->catboost)\n",
      "  Downloading narwhals-1.24.1-py3-none-any.whl.metadata (10.0 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2>=2.10.3->distributed>=2.4.0->dask-ml) (3.0.2)\n",
      "Downloading dask-2025.1.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading dask_ml-2024.4.4-py3-none-any.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.8/149.8 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading lightgbm-4.5.0-py3-none-manylinux_2_28_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading catboost-1.2.7-cp312-cp312-manylinux2014_x86_64.whl (98.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading dask_glm-0.3.2-py2.py3-none-any.whl (13 kB)\n",
      "Downloading distributed-2025.1.0-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached multipledispatch-1.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading numba-0.61.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading partd-1.4.2-py3-none-any.whl (18 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached toolz-1.0.0-py3-none-any.whl (56 kB)\n",
      "Downloading graphviz-0.20.3-py3-none-any.whl (47 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading plotly-6.0.0-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading llvmlite-0.44.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading locket-1.0.0-py2.py3-none-any.whl (4.4 kB)\n",
      "Downloading msgpack-1.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (401 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.4/401.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "Downloading narwhals-1.24.1-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.5/309.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading pyarrow-19.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading sparse-0.15.5-py2.py3-none-any.whl (117 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.4/117.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Downloading tblib-3.0.0-py3-none-any.whl (12 kB)\n",
      "Downloading zict-3.0.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: sortedcontainers, multipledispatch, zict, tqdm, toolz, tblib, pyarrow, numpy, narwhals, msgpack, locket, llvmlite, graphviz, fsspec, cloudpickle, plotly, partd, numba, sparse, lightgbm, dask, pynndescent, distributed, catboost, umap-learn, dask-glm, dask-ml\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.2\n",
      "    Uninstalling numpy-2.2.2:\n",
      "      Successfully uninstalled numpy-2.2.2\n",
      "Successfully installed catboost-1.2.7 cloudpickle-3.1.1 dask-2025.1.0 dask-glm-0.3.2 dask-ml-2024.4.4 distributed-2025.1.0 fsspec-2024.12.0 graphviz-0.20.3 lightgbm-4.5.0 llvmlite-0.44.0 locket-1.0.0 msgpack-1.1.0 multipledispatch-1.0.0 narwhals-1.24.1 numba-0.61.0 numpy-1.26.4 partd-1.4.2 plotly-6.0.0 pyarrow-19.0.0 pynndescent-0.5.13 sortedcontainers-2.4.0 sparse-0.15.5 tblib-3.0.0 toolz-1.0.0 tqdm-4.67.1 umap-learn-0.5.7 zict-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install dask dask-ml umap-learn lightgbm catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tgDJqcZ0rsup"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from dask_ml.model_selection import train_test_split, GridSearchCV\n",
    "from dask_ml.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA, FastICA, FactorAnalysis\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "#from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, Lasso\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "#from umap import UMAP\n",
    "from sklearn.manifold import TSNE, Isomap, LocallyLinearEmbedding\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFE\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "import itertools\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = df.is_click\n",
    "X = df.drop(columns = ['is_click'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X7Fpsn_6y3d-",
    "outputId": "bfd30a14-a2ad-427c-fe2c-8ba17d9fa0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest-PCA-StandardScaler\n",
      "Skipping existing combination: RandomForest-PCA-StandardScaler\n",
      "ExtraTrees-PCA-StandardScaler\n",
      "Skipping existing combination: ExtraTrees-PCA-StandardScaler\n",
      "GradientBoosting-PCA-StandardScaler\n",
      "Skipping existing combination: GradientBoosting-PCA-StandardScaler\n",
      "LogisticRegression-PCA-StandardScaler\n",
      "Skipping existing combination: LogisticRegression-PCA-StandardScaler\n",
      "NaiveBayes-PCA-StandardScaler\n",
      "Skipping existing combination: NaiveBayes-PCA-StandardScaler\n",
      "DecisionTree-PCA-StandardScaler\n",
      "Skipping existing combination: DecisionTree-PCA-StandardScaler\n",
      "KNN-PCA-StandardScaler\n",
      "Skipping existing combination: KNN-PCA-StandardScaler\n",
      "                                    Model  F1 Score\n",
      "0         RandomForest-PCA-StandardScaler  0.656374\n",
      "1                  KNN-PCA-StandardScaler  0.652931\n",
      "2           ExtraTrees-PCA-StandardScaler  0.636903\n",
      "3           ExtraTrees-PCA-StandardScaler  0.635543\n",
      "4           ExtraTrees-PCA-StandardScaler  0.635129\n",
      "5           ExtraTrees-PCA-StandardScaler  0.635042\n",
      "6           ExtraTrees-PCA-StandardScaler  0.625580\n",
      "7         DecisionTree-PCA-StandardScaler  0.574586\n",
      "8             LightGBM-PCA-StandardScaler  0.561238\n",
      "9         RandomForest-PCA-StandardScaler  0.526316\n",
      "10        RandomForest-PCA-StandardScaler  0.525889\n",
      "11        RandomForest-PCA-StandardScaler  0.524609\n",
      "12        RandomForest-PCA-StandardScaler  0.447757\n",
      "13        RandomForest-PCA-StandardScaler  0.447516\n",
      "14        RandomForest-PCA-StandardScaler  0.446898\n",
      "15        RandomForest-PCA-StandardScaler  0.445710\n",
      "16    GradientBoosting-PCA-StandardScaler  0.178820\n",
      "17    GradientBoosting-PCA-StandardScaler  0.105578\n",
      "18  LogisticRegression-PCA-StandardScaler  0.000000\n",
      "19          NaiveBayes-PCA-StandardScaler  0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define Scalers\n",
    "scalers = {\n",
    "    \"StandardScaler\": StandardScaler()\n",
    "}\n",
    "\n",
    "# Define Classifiers\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=30,max_features=10,warm_start=True,max_depth = 15, n_jobs=-1),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, n_jobs=-1),\n",
    "    #\"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1),\n",
    "    #\"LightGBM\": LGBMClassifier(n_jobs=-1),\n",
    "    #\"CatBoost\": CatBoostClassifier(verbose=0),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    #\"SVM_RBF\": SVC(kernel=\"rbf\"),\n",
    "    #\"SVM_Poly\": SVC(kernel=\"poly\"),\n",
    "    #\"SVM_Sigmoid\": SVC(kernel=\"sigmoid\"),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
    "    #\"SGDClassifier\": SGDClassifier(),\n",
    "    \"NaiveBayes\": GaussianNB(),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "}\n",
    "\n",
    "# Define Dimensionality Reduction Methods\n",
    "dim_reductions = {\n",
    "    \"PCA\": PCA(n_components=5),\n",
    "    #\"KPCA_RBF\": KernelPCA(n_components=10, kernel=\"rbf\", n_jobs=-1),\n",
    "    #\"KPCA_Poly\": KernelPCA(n_components=10, kernel=\"poly\", n_jobs=-1),\n",
    "    #\"UMAP\": UMAP(n_components=10),\n",
    "    #\"t-SNE\": TSNE(n_components=10),\n",
    "    #\"Isomap\": Isomap(n_components=10),\n",
    "    #\"LaplacianEigenmaps\": LocallyLinearEmbedding(n_components=10),\n",
    "    #\"FactorAnalysis\": FactorAnalysis(n_components=10),\n",
    "    #\"ICA\": FastICA(n_components=10),\n",
    "}\n",
    "\n",
    "# Define Feature Selection Methods\n",
    "feature_selectors = {\n",
    "    \"MutualInfo\": SelectKBest(score_func=mutual_info_classif, k=20),\n",
    "    #\"RFE (RandomForest)\": RFE(estimator=RandomForestClassifier(n_estimators=50, n_jobs=-1), n_features_to_select=20),\n",
    "    #\"RFE (XGBoost)\": RFE(estimator=XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", n_jobs=-1), n_features_to_select=20),\n",
    "    #\"Lasso\": SelectKBest(score_func=lambda X, y: np.abs(Lasso(alpha=0.01).fit(X, y).coef_), k=20)\n",
    "}\n",
    "\n",
    "# Define Feature Engineering Methods\n",
    "feature_engineering = {\n",
    "    \"PolynomialFeatures\": PolynomialFeatures(degree=2, include_bias=False),\n",
    "    #\"FeatureInteraction\": FunctionTransformer(lambda X: X[:, :10] * X[:, 10:20]),  # Example interaction\n",
    "}\n",
    "\n",
    "# Calculate Total Combinations\n",
    "total_combinations = (\n",
    "    len(models) * len(dim_reductions) * len(scalers) * len(feature_selectors) * len(feature_engineering)\n",
    ")\n",
    "completed_tasks = 0  # Counter for completed tasks\n",
    "\n",
    "# Run Experiments in Parallel with Progress Tracker\n",
    "results = []\n",
    "results_file = \"results.pkl\"\n",
    "if os.path.exists(results_file):\n",
    "    with open(results_file, \"rb\") as f:\n",
    "        results = pickle.load(f)\n",
    "\n",
    "existing_combinations = {entry[0] for entry in results}\n",
    "\n",
    "param_grids = {\n",
    "    \"RandomForest\": {\n",
    "        \"classifier__n_estimators\": [10, 30, 50],\n",
    "        \"classifier__max_features\": [5, 10],\n",
    "        \"classifier__max_depth\": [10, 15, 20],\n",
    "        \"classifier__class_weight\": [\"balanced\"],\n",
    "    },\n",
    "    \"ExtraTrees\": {\n",
    "        \"classifier__n_estimators\": [50, 100, 150],\n",
    "        \"classifier__max_features\": [5, 10],\n",
    "        \"classifier__max_depth\": [10, 20, None],\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"classifier__learning_rate\": [0.01, 0.1, 0.3],\n",
    "        \"classifier__n_estimators\": [50, 100, 150],\n",
    "        \"classifier__max_depth\": [3, 6, 9],\n",
    "    },\n",
    "    \"LightGBM\": {\n",
    "        \"classifier__learning_rate\": [0.01, 0.1],\n",
    "        \"classifier__n_estimators\": [50, 100, 150],\n",
    "        \"classifier__num_leaves\": [15, 31, 63],\n",
    "    },\n",
    "    \"GradientBoosting\": {\n",
    "        \"classifier__learning_rate\": [0.01, 0.1],\n",
    "        #\"classifier__n_estimators\": [50, 100, 150],\n",
    "        #\"classifier__max_depth\": [3, 6, 9],\n",
    "    },\n",
    "    \"SVM_RBF\": {\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__gamma\": [\"scale\", \"auto\"],\n",
    "    },\n",
    "    \"SVM_Poly\": {\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__degree\": [2, 3, 4],\n",
    "        \"classifier__gamma\": [\"scale\", \"auto\"],\n",
    "    },\n",
    "    \"SVM_Sigmoid\": {\n",
    "        \"classifier__C\": [0.1, 1, 10],\n",
    "        \"classifier__gamma\": [\"scale\", \"auto\"],\n",
    "    },\n",
    "    \"LogisticRegression\": {\n",
    "        \"classifier__C\": [0.01, 0.1, 1, 10],\n",
    "        \"classifier__penalty\": [\"l2\"],\n",
    "    },\n",
    "    \"NaiveBayes\": {},\n",
    "    \"DecisionTree\": {\n",
    "        \"classifier__max_depth\": [5, 10, 15, None],\n",
    "        #\"classifier__criterion\": [\"gini\", \"entropy\"],\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"classifier__n_neighbors\": [3, 5, 7],\n",
    "        #\"classifier__weights\": [\"uniform\", \"distance\"],\n",
    "    },\n",
    "}\n",
    "#param_grids = {\n",
    " #   \"RandomForest\": {\"classifier__class_weight\": [\"balanced\"]},\n",
    "  #  \"GradientBoosting\": {\"classifier__learning_rate\": [0.01, 0.1]},\n",
    "   # \"SVM_RBF\": {\"classifier__C\": [0.1, 1, 10]},\n",
    "    #\"SVM_Poly\": {\"classifier__C\": [0.1, 1, 10]},\n",
    "    #\"SVM_Sigmoid\": {\"classifier__C\": [0.1, 1, 10]},\n",
    "    #\"LogisticRegression\": {\"classifier__C\": [0.1, 1, 10]},\n",
    "    #\"NaiveBayes\": {}, \n",
    "    #\"DecisionTree\": {\"classifier__max_depth\": [5, 10, 15]},\n",
    "    #\"KNN\": {\"classifier__n_neighbors\": [3, 5, 7]},\n",
    "#}\n",
    "        \n",
    "for model_name, model in models.items():\n",
    "    for dr_name, dr in dim_reductions.items():\n",
    "        for scaler_name, scaler in scalers.items():\n",
    "                    combination = f\"{model_name}-{dr_name}-{scaler_name}\"\n",
    "                    print(combination)\n",
    "        \n",
    "                    # Skip if the combination already exists\n",
    "                    if combination in existing_combinations:\n",
    "                        print(f\"Skipping existing combination: {combination}\")\n",
    "                        continue\n",
    "                \n",
    "                    pipeline = Pipeline([\n",
    "                        (\"scaler\", scaler),\n",
    "                        (\"dim_red\", dr),\n",
    "                        (\"classifier\", model),\n",
    "                    ])\n",
    "                    \n",
    "                    param_grid = param_grids.get(model_name, {})\n",
    "                    best_f1 = -1\n",
    "\n",
    "                    # Dask-ML GridSearch (Parallel) - Now Optimizing for F1-score\n",
    "                    for params in [dict(zip(param_grid, v)) for v in itertools.product(*param_grid.values())]:\n",
    "                        pipeline.set_params(**params)\n",
    "                        pipeline.fit(X_train, y_train)\n",
    "                        y_pred = pipeline.predict(X_test)\n",
    "                        f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "                        if f1 > best_f1:\n",
    "                            best_f1 = f1\n",
    "                            \n",
    "                    results.append((f\"{model_name}-{dr_name}-{scaler_name}\", best_f1))\n",
    "\n",
    "                    with open(\"results.pkl\", \"wb\") as f:\n",
    "                        pickle.dump(results, f)\n",
    "\n",
    "                    # Update Progress\n",
    "                    completed_tasks += 1\n",
    "                    remaining_tasks = total_combinations - completed_tasks\n",
    "                    print(f\"Progress: {completed_tasks}/{total_combinations}\")\n",
    "\n",
    "# Sort and Display Best Results\n",
    "results.sort(key=lambda x: x[1], reverse=True)\n",
    "import pandas as pd\n",
    "df_results = pd.DataFrame(results, columns=[\"Model\", \"F1 Score\"])\n",
    "print(df_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmrDO8r94I5C"
   },
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
