{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latest & Graeates - Preprocessing, Feature Engineering, Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import holidays\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from itertools import product\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold,\n",
    "    cross_val_score,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing the initial EDA (which can be found in the EDA notebook), we will now perform the following steps:\n",
    "1. Preprocess the data: data impuation, data transformation, data normalization\n",
    "2. Feature addition: add lots of new features (interaction, encoding, binning...) \n",
    "3. Feature selection- Statistical validation tests: perform statistical tests to see if the new features are useful.\n",
    "4. Feature selection- cross validation tests: perform cross validation to see if the new features are useful.\n",
    "5. Model selection: try different models and see which one performs the best.\n",
    "6. Model tuning: tune the best model to see if we can improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading & cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and clean the data\n",
    "\n",
    "filepath = \"train_dataset_full.csv\"\n",
    "df = pd.read_csv(filepath)\n",
    "\n",
    "# clean the data\n",
    "def clean_data(df):\n",
    "    # remove entirely empty rows and fully duplicate rows\n",
    "    df = df.dropna(how=\"all\").drop_duplicates()\n",
    "    \n",
    "    # ensure the DateTime column is in datetime format\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "    # ensure values that are supposed to be ints are indeed so (and not unnecessarily floats)\n",
    "    int_columns = ['campaign_id', 'webpage_id', 'product_category_1',\n",
    "                   'age_level', 'user_depth', 'city_development_index']\n",
    "    for col in int_columns:\n",
    "        df[col] = df[col].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data imputation\n",
    "\n",
    "In the EDA, the missing values that stood out were in:\n",
    "1. \"product_category_2\" (79.16% missing): we will creates a missing indicator feature (which might get dropped in future steps if deemed irrelevant) but won't impute values, as that could introduce misleading data, as there's simply too much missing data. \n",
    "2. \"city_development_index\" (27.60%): we will creates a missing indicator feature (which might get dropped in future steps if deemed irrelevant) and impute with the mode.\n",
    "3. \"gender\", \"age_level\", \"user_depth\", \"user_group_id\": we will first attempt to fill from other sessions of the same user, but we'll fallback to global defaults as needed.\n",
    "4. Else: we will automatically impute missing values for features with missing rates below 1%, using the mode for categorical variables (including those with low cardinality), and medians for numerical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_demographics_by_user(df):\n",
    "    \"\"\"\n",
    "    Imputes demographic features (gender, age_level, user_depth) using other \n",
    "    sessions from the same user when available, then falls back to global defaults.\n",
    "    \n",
    "    The EDA showed these features have about 4.7% missing values and are generally\n",
    "    consistent within users, making this a reliable approach.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # first, attempt to fill missing demographics using other sessions from same user\n",
    "    demographics = ['gender', 'age_level', 'user_depth', 'user_group_id']\n",
    "    \n",
    "    for demo in demographics:\n",
    "        # group by user_id and apply forward fill and backward fill\n",
    "        df[demo] = df.groupby('user_id')[demo].transform(\n",
    "            lambda x: x.ffill().bfill()\n",
    "        )\n",
    "    \n",
    "    # if there are any remaining missing values, fill with mode values:\n",
    "    defaults = {\"gender\": df[\"gender\"].mode().iloc[0],\n",
    "                \"age_level\": df[\"age_level\"].mode().iloc[0],\n",
    "                \"user_depth\": df[\"user_depth\"].mode().iloc[0],\n",
    "                \"user_group_id\": df[\"user_group_id\"].mode().iloc[0]}\n",
    "\n",
    "    \n",
    "    for column, default in defaults.items():\n",
    "        df[column] = df[column].fillna(default)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_product_category2(df):\n",
    "    \"\"\"\n",
    "    Handles product_category_2 which has 79.16% missing values.\n",
    "    Creates a binary indicator for missingness and leaves original values as is.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create missing indicator\n",
    "    df['product_category_2_missing'] = df['product_category_2'].isna().astype(int)\n",
    "    \n",
    "    # return without imputing due to extremely high missingness\n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_city_development(df):\n",
    "    \"\"\"\n",
    "    Handles city_development_index which has 27.60% missing values.\n",
    "    Creates a missing indicator and imputes with mode.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # create missing indicator\n",
    "    df['city_development_missing'] = df['city_development_index'].isna().astype(int)\n",
    "    \n",
    "    # impute with mode\n",
    "    city_development_mode = df['city_development_index'].mode()[0]\n",
    "    df['city_development_index'] = df['city_development_index'].fillna(city_development_mode)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def handle_low_missing(df, threshold: float=0.01):\n",
    "    \"\"\"\n",
    "    Handles features with low missing rates (< threshold).\n",
    "    Uses mode for categorical variables and median for numerical ones.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # identify columns with low missing rates\n",
    "    missing_rates = df.isnull().mean()\n",
    "    low_missing_cols = missing_rates[missing_rates > 0][missing_rates < threshold].index\n",
    "    \n",
    "    for col in low_missing_cols:\n",
    "        # determine if column should be treated as categorical\n",
    "        unique_vals = df[col].nunique()\n",
    "        is_categorical = pd.api.types.is_object_dtype(df[col]) or unique_vals <= 10\n",
    "        \n",
    "        if is_categorical:\n",
    "            # for categorical variables, use mode\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "        else:\n",
    "            # for numerical variables, use median\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def impute_dataset(df):\n",
    "    \"\"\"\n",
    "    Applies the complete imputation pipeline to the dataset.\n",
    "    Returns a new dataframe with all imputation strategies applied.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # apply each imputation step in sequence\n",
    "    df = impute_demographics_by_user(df)\n",
    "    df = handle_product_category2(df)\n",
    "    df = handle_city_development(df)\n",
    "    df = handle_low_missing(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_imputed = impute_dataset(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation of the imputation: looking at column distributions changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "session_id:\n",
      "Mean - Original: 285452.08, Imputed: 285451.90\n",
      "Std  - Original: 168593.35, Imputed: 167914.47\n",
      "\n",
      "user_id:\n",
      "Mean - Original: 545905.09, Imputed: 545790.67\n",
      "Std  - Original: 329529.65, Imputed: 328225.35\n",
      "\n",
      "product distribution:\n",
      "Original vs Imputed:\n",
      "         Original   Imputed\n",
      "product                    \n",
      "C        0.353391  0.358681\n",
      "H        0.235999  0.234069\n",
      "I        0.137374  0.136250\n",
      "D        0.088477  0.087753\n",
      "B        0.048436  0.048040\n",
      "\n",
      "campaign_id:\n",
      "Mean - Original: 308547.20, Imputed: 308968.33\n",
      "Std  - Original: 126502.49, Imputed: 126063.30\n",
      "\n",
      "webpage_id:\n",
      "Mean - Original: 29699.44, Imputed: 29569.53\n",
      "Std  - Original: 21548.50, Imputed: 21508.07\n",
      "\n",
      "product_category_1:\n",
      "Mean - Original: 3.07, Imputed: 3.08\n",
      "Std  - Original: 1.30, Imputed: 1.30\n",
      "\n",
      "user_group_id:\n",
      "Mean - Original: 3.48, Imputed: 3.46\n",
      "Std  - Original: 2.42, Imputed: 2.36\n",
      "\n",
      "gender distribution:\n",
      "Original vs Imputed:\n",
      "        Original   Imputed\n",
      "gender                    \n",
      "Male    0.883568  0.889101\n",
      "Female  0.116432  0.110899\n",
      "\n",
      "age_level:\n",
      "Mean - Original: 2.78, Imputed: 2.79\n",
      "Std  - Original: 1.07, Imputed: 1.05\n",
      "\n",
      "user_depth:\n",
      "Mean - Original: 2.88, Imputed: 2.88\n",
      "Std  - Original: 0.40, Imputed: 0.39\n",
      "\n",
      "city_development_index:\n",
      "Mean - Original: 2.56, Imputed: 2.40\n",
      "Std  - Original: 0.92, Imputed: 0.82\n",
      "\n",
      "var_1:\n",
      "Mean - Original: 0.42, Imputed: 0.42\n",
      "Std  - Original: 0.49, Imputed: 0.49\n",
      "\n",
      "is_click:\n",
      "Mean - Original: 0.07, Imputed: 0.07\n",
      "Std  - Original: 0.25, Imputed: 0.25\n"
     ]
    }
   ],
   "source": [
    "def validate_imputation(df_original, df_imputed):\n",
    "    \"\"\"Validates that imputation maintained reasonable distributions\"\"\"\n",
    "    for col in df_original.columns:\n",
    "        if col == 'product_category_2':  # skip intentionally non-imputed column\n",
    "            continue\n",
    "            \n",
    "        if pd.api.types.is_numeric_dtype(df_original[col]):\n",
    "            # check that means and stds are similar for numeric columns\n",
    "            orig_mean = df_original[col].mean()\n",
    "            imp_mean = df_imputed[col].mean()\n",
    "            orig_std = df_original[col].std()\n",
    "            imp_std = df_imputed[col].std()\n",
    "            \n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Mean - Original: {orig_mean:.2f}, Imputed: {imp_mean:.2f}\")\n",
    "            print(f\"Std  - Original: {orig_std:.2f}, Imputed: {imp_std:.2f}\")\n",
    "        \n",
    "        elif pd.api.types.is_object_dtype(df_original[col]):\n",
    "            # check value distributions for categorical columns\n",
    "            orig_dist = df_original[col].value_counts(normalize=True)\n",
    "            imp_dist = df_imputed[col].value_counts(normalize=True)\n",
    "            \n",
    "            print(f\"\\n{col} distribution:\")\n",
    "            print(\"Original vs Imputed:\")\n",
    "            print(pd.concat([orig_dist, imp_dist], axis=1, \n",
    "                          keys=['Original', 'Imputed']).head())\n",
    "            \n",
    "\n",
    "imputation_params = {\n",
    "    'demographics_defaults': {\n",
    "        'gender': 'Male',\n",
    "        'age_level': 3.0,\n",
    "        'user_depth': 3.0,\n",
    "        'user_group_id': 3.0\n",
    "    },\n",
    "    'city_development_mode': 2.0\n",
    "}\n",
    "\n",
    "# save for later use\n",
    "with open('imputation_params.json', 'w') as f:\n",
    "    json.dump(imputation_params, f)\n",
    "\n",
    "\n",
    "def print_imputation_summary(df_original, df_imputed):\n",
    "    \"\"\"Prints summary of imputation changes\"\"\"\n",
    "    total_missing_before = df_original.isnull().sum().sum()\n",
    "    total_missing_after = df_imputed.isnull().sum().sum()\n",
    "    \n",
    "    print(\"\\nImputation Summary:\")\n",
    "    print(f\"Total missing values before: {total_missing_before:,}\")\n",
    "    print(f\"Total missing values after: {total_missing_after:,}\")\n",
    "    print(f\"Total values imputed: {total_missing_before - total_missing_after:,}\")\n",
    "\n",
    "validate_imputation(df, df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the above validation, we can see that:\n",
    "1. Our target feature \"is_click\" maintained exactly same means and std, meaning its integrity is preserved.\n",
    "2. All features maintain very similar proportions apart from the below; \"gender\" ratio slightly shifted but reasonably (Male: 88.3% → 88.9%) but these small shifts are acceptable given the missingness rates.\n",
    "3. \"city_development_index\" shows the largest shift, but it's expected given its high missingness rate (27.6%) and our mode imputation strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_temporal_features(df):\n",
    "    \"\"\"\n",
    "    Creates temporal features from DateTime column without leakage.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # basic time features (these are OK as they don't use future data)\n",
    "    df['hour'] = df['DateTime'].dt.hour\n",
    "    df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "    \n",
    "    # binary features\n",
    "    df['is_business_hours'] = df['hour'].between(9, 17).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_early_morning'] = df['hour'].between(2, 5).astype(int)\n",
    "    \n",
    "    # time of day as categorical\n",
    "    df['time_of_day'] = pd.cut(\n",
    "        df['hour'],\n",
    "        bins=[-np.inf, 6, 12, 18, np.inf],\n",
    "        labels=['night', 'morning', 'afternoon', 'evening']\n",
    "    )\n",
    "    \n",
    "    # add hour bins\n",
    "    df['hour_bin'] = pd.cut(\n",
    "        df['hour'],\n",
    "        bins=[0, 4, 8, 12, 16, 20, 24],\n",
    "        labels=['dawn', 'early_morning', 'morning', 'afternoon', 'evening', 'night'],\n",
    "        include_lowest=True,\n",
    "        right=False\n",
    "    )\n",
    "    \n",
    "    # holiday features\n",
    "    us_holidays = holidays.US()\n",
    "    df['date'] = df['DateTime'].dt.date\n",
    "    df['is_holiday'] = df['date'].apply(\n",
    "        lambda x: bool(us_holidays.get(x)) if pd.notna(x) else 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    # near holiday feature (only using past holidays to prevent leakage)\n",
    "    df['is_near_holiday'] = df.apply(\n",
    "        lambda row: any(\n",
    "            abs((holiday - row['date']).days) <= 2\n",
    "            for holiday in us_holidays.keys()\n",
    "            if holiday <= row['date']  # only consider past and current holidays\n",
    "        ),\n",
    "        axis=1\n",
    "    ).astype(int)\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_user_engagement_features(df):\n",
    "    \"\"\"\n",
    "    Creates user engagement features without leakage by using expanding windows.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # sort by user and time\n",
    "    df = df.sort_values(['user_id', 'DateTime'])\n",
    "    \n",
    "    # calculate historical CTR using expanding window\n",
    "    df['cumulative_clicks'] = df.groupby('user_id')['is_click'].transform(\n",
    "        lambda x: x.shift().expanding().sum()).fillna(0)\n",
    "    df['cumulative_impressions'] = df.groupby('user_id').cumcount()\n",
    "    df['historical_user_ctr'] = (df['cumulative_clicks'] + 1) / (df['cumulative_impressions'] + 2)  # laplace smoothing\n",
    "    \n",
    "    # session counts (using only past data)\n",
    "    df['session_count_user'] = df.groupby('user_id').cumcount() + 1\n",
    "    df['session_count_log'] = np.log1p(df['session_count_user'])\n",
    "    \n",
    "    # sessions per day using a custom cumulative computation\n",
    "    df['date'] = df['DateTime'].dt.date\n",
    "    \n",
    "    def compute_sessions_per_day_mean(dates):\n",
    "        \"\"\"For each row in the dates series, compute cumulative sessions per unique day.\"\"\"\n",
    "        seen = set()\n",
    "        ratios = []\n",
    "        for i, d in enumerate(dates):\n",
    "            seen.add(d)\n",
    "            # (i+1) is the cumulative count; len(seen) is the number of unique days so far\n",
    "            ratios.append((i + 1) / len(seen))\n",
    "        return pd.Series(ratios, index=dates.index)\n",
    "    \n",
    "    df['sessions_per_day_mean'] = df.groupby('user_id')['date'].transform(compute_sessions_per_day_mean)\n",
    "    df['sessions_per_day_mean_log'] = np.log1p(df['sessions_per_day_mean'])\n",
    "    \n",
    "    # time since last click (only using past data)\n",
    "    df['time_since_last_click'] = df.groupby('user_id')['DateTime'].diff().dt.total_seconds() / 3600\n",
    "    df['time_since_last_click'] = df['time_since_last_click'].fillna(168)  # 1 week for first session\n",
    "    \n",
    "    # click frequency in past 24h\n",
    "    df['prev_24h'] = df['DateTime'] - pd.Timedelta(hours=24)\n",
    "    df['click_frequency_24h'] = df.groupby('user_id').apply(\n",
    "        lambda group: group.apply(\n",
    "            lambda row: group[\n",
    "                (group['DateTime'] < row['DateTime']) &  # only use past data\n",
    "                (group['DateTime'] >= row['prev_24h']) & \n",
    "                (group['is_click'] == 1)\n",
    "            ].shape[0],\n",
    "            axis=1\n",
    "        )\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    \n",
    "    df['click_frequency_24h_log'] = np.log1p(df['click_frequency_24h'])\n",
    "    \n",
    "    # user engagement score using percentile ranks of past data only\n",
    "    engagement_features = [\n",
    "        'historical_user_ctr',\n",
    "        'session_count_log',\n",
    "        'sessions_per_day_mean_log',\n",
    "        'click_frequency_24h_log'\n",
    "    ]\n",
    "    \n",
    "    # calculate expanding ranks for each feature\n",
    "    # which means : for each user, calculate the percentile rank of each feature at each time point using only past data of that user\n",
    "    # example: if a user has 10 sessions, the first session will have a rank of 0, the second session will have a rank of 0.1, and so on up to 1\n",
    "    for feature in engagement_features:\n",
    "        df[f'{feature}_rank'] = df.groupby('user_id')[feature].transform(\n",
    "            lambda x: x.shift().expanding().rank(pct=True)).fillna(0)  # first value gets rank 0\n",
    "    \n",
    "    # weighted combination of ranks\n",
    "    df['user_engagement_score'] = (\n",
    "        0.4 * df['historical_user_ctr_rank'] +\n",
    "        0.25 * df['session_count_log_rank'] +\n",
    "        0.25 * df['sessions_per_day_mean_log_rank'] +\n",
    "        0.1 * df['click_frequency_24h_log_rank']\n",
    "    )\n",
    "    \n",
    "    df = df.drop(['prev_24h', 'date', 'cumulative_clicks', 'cumulative_impressions'], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_campaign_performance_features(df):\n",
    "    \"\"\"\n",
    "    Creates campaign performance features using only past data.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # sort by time to ensure we only use past data\n",
    "    df = df.sort_values('DateTime')\n",
    "    \n",
    "    # calculate expanding window statistics for campaigns\n",
    "    df['campaign_impressions'] = df.groupby('campaign_id').cumcount() + 1\n",
    "    df['campaign_clicks'] = df.groupby('campaign_id')['is_click'].expanding().sum().reset_index(0, drop=True)\n",
    "    df['campaign_historical_ctr'] = (df['campaign_clicks'] + 1) / (df['campaign_impressions'] + 2)  # laplace smoothing\n",
    "    df['campaign_historical_ctr_log'] = np.log1p(df['campaign_historical_ctr'] * 100)\n",
    "    \n",
    "    # campaign-webpage combination with relative performance\n",
    "    df['campaign_webpage_clicks'] = (\n",
    "    df.groupby(['campaign_id', 'webpage_id'])['is_click']\n",
    "        .expanding()\n",
    "        .sum()\n",
    "        .reset_index(level=[0, 1], drop=True)  # reset both groupby indices\n",
    "        )\n",
    "    df['campaign_webpage_impressions'] = df.groupby(['campaign_id', 'webpage_id']).cumcount() + 1\n",
    "    df['campaign_webpage_ctr'] = (df['campaign_webpage_clicks'] + 1) / (df['campaign_webpage_impressions'] + 2)\n",
    "    df['campaign_webpage_relative'] = df['campaign_webpage_ctr'] / df['campaign_historical_ctr']\n",
    "    \n",
    "    # hourly performance\n",
    "    df['campaign_hour_clicks'] = (\n",
    "        df.groupby(['campaign_id', 'hour'])['is_click']\n",
    "        .cumsum()\n",
    "        .shift(fill_value=0))\n",
    "    df['campaign_hour_impressions'] = df.groupby(['campaign_id', 'hour']).cumcount() + 1\n",
    "    df['campaign_hour_ctr'] = (df['campaign_hour_clicks'] + 1) / (df['campaign_hour_impressions'] + 2)\n",
    "    df['campaign_hour_relative'] = df['campaign_hour_ctr'] / df['campaign_historical_ctr']\n",
    "    \n",
    "    # campaign success as percentile rank (using expanding window)\n",
    "    df['campaign_success_percentile'] = df.groupby('DateTime')['campaign_historical_ctr'].rank(pct=True)\n",
    "    \n",
    "    # clean up intermediate columns\n",
    "    df = df.drop(['campaign_clicks', 'campaign_webpage_clicks', 'campaign_webpage_impressions',\n",
    "                  'campaign_hour_clicks', 'campaign_hour_impressions', 'campaign_hour_ctr',\n",
    "                  'campaign_webpage_ctr'], axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"\n",
    "    Creates interaction features without leakage.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # time-based interactions\n",
    "    df['campaign_hour_bin'] = df['campaign_id'].astype(str) + '_' + df['hour_bin'].astype(str)\n",
    "    df['campaign_early_morning'] = df['campaign_id'] * df['is_early_morning']\n",
    "    \n",
    "    # user depth interactions\n",
    "    df['user_depth_time'] = df['user_depth'].astype(str) + '_' + df['time_of_day'].astype(str)\n",
    "    df['age_weekend'] = df['age_level'] * df['is_weekend']\n",
    "    df['user_depth_age'] = df['user_depth'] * df['age_level']\n",
    "    \n",
    "    # session-time interaction\n",
    "    df['session_count_bin'] = pd.cut(\n",
    "        df['session_count_log'],\n",
    "        bins=5,  # Fixed 5 bins\n",
    "        labels=['VL', 'L', 'M', 'H', 'VH'])\n",
    "    df['user_sessions_time'] = df['session_count_bin'].astype(str) + '_' + df['time_of_day'].astype(str)\n",
    "    \n",
    "    # demographic interactions\n",
    "    df['gender_age'] = df['gender'].astype(str) + '_' + df['age_level'].astype(str)\n",
    "    \n",
    "    # geographic interactions\n",
    "    df['city_business'] = df['city_development_index'] * df['is_business_hours']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def prepare_dataset_for_inference(df, imputation_params, scaler, selected_features):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for inference using saved parameters.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # first, impute missing values using saved parameters\n",
    "    df['gender'] = df['gender'].fillna(imputation_params['demographics_defaults']['gender'])\n",
    "    df['age_level'] = df['age_level'].fillna(imputation_params['demographics_defaults']['age_level'])\n",
    "    df['user_depth'] = df['user_depth'].fillna(imputation_params['demographics_defaults']['user_depth'])\n",
    "    df['user_group_id'] = df['user_group_id'].fillna(imputation_params['demographics_defaults']['user_group_id'])\n",
    "    df['city_development_index'] = df['city_development_index'].fillna(imputation_params['city_development_mode'])\n",
    "    \n",
    "    # create features\n",
    "    df = create_temporal_features(df)\n",
    "    df = create_user_engagement_features(df)\n",
    "    df = create_campaign_performance_features(df)\n",
    "    df = create_interaction_features(df)\n",
    "    \n",
    "    # scale features using saved scaler\n",
    "    numeric_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_features] = scaler.transform(df[numeric_features])\n",
    "    \n",
    "    # select only the required features in the correct order\n",
    "    return df[selected_features]\n",
    "\n",
    "\n",
    "def create_all_features(df):\n",
    "    \"\"\"\n",
    "    Applies all feature engineering steps and returns both engineered features\n",
    "    and a list of features requiring scaling.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # apply each feature engineering step\n",
    "    df = create_temporal_features(df)\n",
    "    df = create_user_engagement_features(df)\n",
    "    df = create_campaign_performance_features(df)\n",
    "    df = create_interaction_features(df)\n",
    "    \n",
    "    # identify features that need scaling\n",
    "    features_to_scale = [\n",
    "        'historical_user_ctr',\n",
    "        'session_count_log',\n",
    "        'sessions_per_day_mean',\n",
    "        'sessions_per_day_std',\n",
    "        'session_count_user',\n",
    "        'time_since_last_click',\n",
    "        'click_frequency_24h',\n",
    "        'click_frequency_24h_log',\n",
    "        'user_engagement_score',\n",
    "        'campaign_historical_ctr_log',\n",
    "        'campaign_webpage_relative',\n",
    "        'campaign_hour_relative',\n",
    "        'campaign_success_percentile',\n",
    "        'user_depth_age',\n",
    "        'age_weekend',\n",
    "        'city_business'\n",
    "        ]\n",
    "    \n",
    "    return df, features_to_scale\n",
    "\n",
    "\n",
    "df_engineered, features_to_scale = create_all_features(df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the completness of the new features, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Validation Results:\n",
      "--------------------------------------------------\n",
      "\n",
      "Features with null values:\n",
      "  product_category_2: 292615 nulls\n",
      "\n",
      "Features with very low variance:\n",
      "  campaign_historical_ctr: 0.000388\n",
      "  campaign_webpage_relative: 0.003562\n",
      "\n",
      "CTR features with invalid ranges:\n",
      "  campaign_historical_ctr_log: range [0.981, 4.215]\n"
     ]
    }
   ],
   "source": [
    "def validate_engineered_features(df):\n",
    "    \"\"\"\n",
    "    Performs sanity checks on engineered features.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # check for nulls\n",
    "    null_counts = df.isnull().sum()\n",
    "    results['features_with_nulls'] = null_counts[null_counts > 0].to_dict()\n",
    "    \n",
    "    # check for infinite values\n",
    "    inf_counts = df.isin([np.inf, -np.inf]).sum()\n",
    "    results['features_with_inf'] = inf_counts[inf_counts > 0].to_dict()\n",
    "    \n",
    "    # validate binary features are actually binary\n",
    "    binary_features = [col for col in df.columns if col.startswith('is_')]\n",
    "    non_binary = {col: sorted(df[col].unique()) \n",
    "                 for col in binary_features \n",
    "                 if not df[col].isin([0, 1, np.nan]).all()}\n",
    "    results['non_binary_features'] = non_binary\n",
    "    \n",
    "    # check for low variance features\n",
    "    variances = df.select_dtypes(include=np.number).var()\n",
    "    low_variance = variances[variances < 0.01].to_dict()\n",
    "    results['low_variance_features'] = low_variance\n",
    "    \n",
    "    # validate time-based features\n",
    "    if 'hour' in df.columns:\n",
    "        results['hour_range_valid'] = df['hour'].between(0, 23).all()\n",
    "    if 'day_of_week' in df.columns:\n",
    "        results['day_range_valid'] = df['day_of_week'].between(0, 6).all()\n",
    "        \n",
    "    # check for correct CTR ranges (between 0 and 1)\n",
    "    ctr_features = [col for col in df.columns if 'ctr' in col.lower()]\n",
    "    invalid_ctr = {col: (df[col].min(), df[col].max()) \n",
    "                  for col in ctr_features \n",
    "                  if not df[col].between(0, 1, inclusive='both').all()}\n",
    "    results['invalid_ctr_ranges'] = invalid_ctr\n",
    "    \n",
    "    # cardinality check for categorical features\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    high_cardinality = {col: df[col].nunique() \n",
    "                       for col in categorical_cols \n",
    "                       if df[col].nunique() > 100}\n",
    "    results['high_cardinality_features'] = high_cardinality\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_validation_results(results: dict):\n",
    "    \"\"\"\n",
    "    Prints validation results in a readable format.\n",
    "    \"\"\"\n",
    "    print(\"Feature Validation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    if results['features_with_nulls']:\n",
    "        print(\"\\nFeatures with null values:\")\n",
    "        for feat, count in results['features_with_nulls'].items():\n",
    "            print(f\"  {feat}: {count} nulls\")\n",
    "    \n",
    "    if results['features_with_inf']:\n",
    "        print(\"\\nFeatures with infinite values:\")\n",
    "        for feat, count in results['features_with_inf'].items():\n",
    "            print(f\"  {feat}: {count} infinities\")\n",
    "    \n",
    "    if results['non_binary_features']:\n",
    "        print(\"\\nNon-binary 'is_' features:\")\n",
    "        for feat, values in results['non_binary_features'].items():\n",
    "            print(f\"  {feat}: {values}\")\n",
    "    \n",
    "    if results['low_variance_features']:\n",
    "        print(\"\\nFeatures with very low variance:\")\n",
    "        for feat, var in results['low_variance_features'].items():\n",
    "            print(f\"  {feat}: {var:.6f}\")\n",
    "    \n",
    "    if results['invalid_ctr_ranges']:\n",
    "        print(\"\\nCTR features with invalid ranges:\")\n",
    "        for feat, (min_val, max_val) in results['invalid_ctr_ranges'].items():\n",
    "            print(f\"  {feat}: range [{min_val:.3f}, {max_val:.3f}]\")\n",
    "    \n",
    "    if results['high_cardinality_features']:\n",
    "        print(\"\\nHigh cardinality categorical features:\")\n",
    "        for feat, count in results['high_cardinality_features'].items():\n",
    "            print(f\"  {feat}: {count} unique values\")\n",
    "\n",
    "\n",
    "validation_results = validate_engineered_features(df_engineered)\n",
    "print_validation_results(validation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: These columns are not categorized: {'DateTime', 'product_category_2', 'user_id', 'session_id'}\n"
     ]
    }
   ],
   "source": [
    "def prepare_features_for_modeling(df, categorical_features, features_to_scale, keep_as_is):\n",
    "    \"\"\"\n",
    "    Prepares features for modeling by handling categorical and numerical variables.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # verify all features are accounted for\n",
    "    all_features = set(categorical_features + features_to_scale + keep_as_is)\n",
    "    missing_features = set(df.columns) - all_features\n",
    "    extra_features = all_features - set(df.columns)\n",
    "    \n",
    "    if missing_features:\n",
    "        print(f\"Warning: These columns are not categorized: {missing_features}\")\n",
    "    if extra_features:\n",
    "        print(f\"Warning: These categorized features are not in dataframe: {extra_features}\")\n",
    "\n",
    "    # convert boolean columns to integers\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "    \n",
    "    # ordinal encoding for ordinal features\n",
    "    ordinal_features = {\n",
    "        'age_level': range(7),\n",
    "        'user_depth': range(1, 4),\n",
    "        'city_development_index': range(1, 5)\n",
    "    }\n",
    "    \n",
    "    ordinal_feature_names = []\n",
    "    for col, categories in ordinal_features.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.Categorical(df[col], categories=categories, ordered=True).codes\n",
    "            ordinal_feature_names.append(col)\n",
    "    \n",
    "    # one-hot encoding for categorical features\n",
    "    encoded_feature_names = []\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True, dtype=float)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df = df.drop(col, axis=1)\n",
    "            encoded_feature_names.extend(dummies.columns.tolist())\n",
    "\n",
    "    # update categorical_features to include both ordinal and one-hot encoded features\n",
    "    categorical_features = ordinal_feature_names + encoded_feature_names\n",
    "\n",
    "    # remove features not needed for modeling\n",
    "    # true id features (that are not basically categories); datetime; \"product_category_2\" (due to high missingness)\n",
    "    df = df.drop(['session_id', 'user_id', 'DateTime', 'product_category_2'], axis=1)            \n",
    "    \n",
    "    return df, categorical_features\n",
    "\n",
    "\n",
    "categorical_features = [\n",
    "    # base categorical features\n",
    "    'gender', \n",
    "    'product',\n",
    "    \n",
    "    # temporal categoricals\n",
    "    'time_of_day',    # night/morning/afternoon/evening\n",
    "    'hour_bin',       # dawn/early_morning/morning/afternoon/evening/night\n",
    "    \n",
    "    # binned features\n",
    "    'session_count_bin',  # VL/L/M/H/VH\n",
    "    \n",
    "    # interaction categoricals\n",
    "    'campaign_hour_bin',  # campaign_id + hour_bin\n",
    "    'user_depth_time',    # user_depth + time_of_day\n",
    "    'user_sessions_time', # session_count_bin + time_of_day\n",
    "    'gender_age'         # gender + age_level\n",
    "]\n",
    "\n",
    "features_to_scale = [\n",
    "    # user engagement metrics\n",
    "    'historical_user_ctr',\n",
    "    'session_count_log',\n",
    "    'sessions_per_day_mean',\n",
    "    'sessions_per_day_mean_log',\n",
    "    'session_count_user',\n",
    "    'time_since_last_click',\n",
    "    'click_frequency_24h',\n",
    "    'click_frequency_24h_log',\n",
    "    'user_engagement_score',\n",
    "    \n",
    "    # ranking features\n",
    "    'historical_user_ctr_rank',\n",
    "    'session_count_log_rank',\n",
    "    'sessions_per_day_mean_log_rank',\n",
    "    'click_frequency_24h_log_rank',\n",
    "    \n",
    "    # campaign performance metrics\n",
    "    'campaign_historical_ctr',\n",
    "    'campaign_historical_ctr_log',\n",
    "    'campaign_webpage_relative',\n",
    "    'campaign_hour_relative',\n",
    "    'campaign_success_percentile', \n",
    "        \n",
    "    # interaction numerics\n",
    "    'user_depth_age',\n",
    "    'age_weekend',\n",
    "    'city_business',\n",
    "    'campaign_early_morning'\n",
    "]\n",
    "\n",
    "keep_as_is = [\n",
    "    'campaign_id',\n",
    "    'webpage_id',\n",
    "    'is_click',  # target, will be later separated\n",
    "    \n",
    "    # binary features\n",
    "    'is_business_hours',\n",
    "    'is_weekend',\n",
    "    'is_early_morning',\n",
    "    'is_holiday',\n",
    "    'is_near_holiday',\n",
    "    'product_category_2_missing',\n",
    "    'city_development_missing',\n",
    "    'var_1',\n",
    "    \n",
    "    # time features in fixed ranges\n",
    "    'hour',         # 0-23\n",
    "    'day_of_week',  # 0-6\n",
    "    \n",
    "    # ordinal features\n",
    "    'age_level',           # 0-6\n",
    "    'user_depth',          # 1-3\n",
    "    'city_development_index', # 1-4\n",
    "    \n",
    "    # other features\n",
    "    'product_category_1',  # 1-5\n",
    "    'user_group_id',\n",
    "    'campaign_impressions'\n",
    "]\n",
    "\n",
    "\n",
    "# prepare the features for modeling by encoding categorical variables\n",
    "df_prepared, categorical_features = prepare_features_for_modeling(\n",
    "    df_engineered, \n",
    "    categorical_features, \n",
    "    features_to_scale, \n",
    "    keep_as_is\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with 164 features\n",
      "\n",
      "Low variance features removed (43):\n",
      "- campaign_historical_ctr (variance: 0.000388)\n",
      "- campaign_webpage_relative (variance: 0.003562)\n",
      "- session_count_bin_H (variance: 0.001591)\n",
      "- session_count_bin_VH (variance: 0.006050)\n",
      "- campaign_hour_bin_105960.0_dawn (variance: 0.002765)\n",
      "- campaign_hour_bin_105960.0_evening (variance: 0.008292)\n",
      "- campaign_hour_bin_105960.0_night (variance: 0.005713)\n",
      "- campaign_hour_bin_118601.0_dawn (variance: 0.002693)\n",
      "- campaign_hour_bin_359520.0_dawn (variance: 0.005111)\n",
      "- campaign_hour_bin_360936.0_dawn (variance: 0.003241)\n",
      "- campaign_hour_bin_396664.0_afternoon (variance: 0.009901)\n",
      "- campaign_hour_bin_396664.0_dawn (variance: 0.001898)\n",
      "- campaign_hour_bin_396664.0_early_morning (variance: 0.007651)\n",
      "- campaign_hour_bin_396664.0_morning (variance: 0.008215)\n",
      "- campaign_hour_bin_404347.0_dawn (variance: 0.002617)\n",
      "- campaign_hour_bin_404347.0_early_morning (variance: 0.009052)\n",
      "- campaign_hour_bin_405490.0_dawn (variance: 0.002975)\n",
      "- campaign_hour_bin_414149.0_dawn (variance: 0.000216)\n",
      "- campaign_hour_bin_414149.0_early_morning (variance: 0.006098)\n",
      "- campaign_hour_bin_414149.0_night (variance: 0.008462)\n",
      "- campaign_hour_bin_82320.0_dawn (variance: 0.000157)\n",
      "- campaign_hour_bin_82320.0_early_morning (variance: 0.003800)\n",
      "- campaign_hour_bin_98970.0_dawn (variance: 0.000406)\n",
      "- campaign_hour_bin_98970.0_early_morning (variance: 0.009180)\n",
      "- user_depth_time_1.0_evening (variance: 0.005948)\n",
      "- user_depth_time_1.0_morning (variance: 0.008422)\n",
      "- user_depth_time_1.0_night (variance: 0.002698)\n",
      "- user_depth_time_2.0_night (variance: 0.007445)\n",
      "- user_sessions_time_H_evening (variance: 0.000438)\n",
      "- user_sessions_time_H_morning (variance: 0.000460)\n",
      "- user_sessions_time_H_night (variance: 0.000157)\n",
      "- user_sessions_time_M_afternoon (variance: 0.006736)\n",
      "- user_sessions_time_M_evening (variance: 0.004154)\n",
      "- user_sessions_time_M_morning (variance: 0.006787)\n",
      "- user_sessions_time_M_night (variance: 0.001912)\n",
      "- user_sessions_time_VH_afternoon (variance: 0.000562)\n",
      "- user_sessions_time_VH_evening (variance: 0.004586)\n",
      "- user_sessions_time_VH_morning (variance: 0.000660)\n",
      "- user_sessions_time_VH_night (variance: 0.000257)\n",
      "- gender_age_Female_1.0 (variance: 0.003091)\n",
      "- gender_age_Female_6.0 (variance: 0.002283)\n",
      "- gender_age_Male_0.0 (variance: 0.000268)\n",
      "- gender_age_Male_6.0 (variance: 0.003749)\n",
      "\n",
      "Highly correlated features removed (5):\n",
      "- session_count_user\n",
      "- click_frequency_24h_log_rank\n",
      "- session_count_log_rank\n",
      "- sessions_per_day_mean_log_rank\n",
      "- user_engagement_score\n",
      "\n",
      "Low mutual information features removed (2):\n",
      "- campaign_impressions (MI score: 0.000403)\n",
      "- campaign_hour_relative (MI score: 0.000181)\n",
      "\n",
      "Statistically insignificant features removed (23):\n",
      "- user_depth\n",
      "- city_development_missing\n",
      "- is_business_hours\n",
      "- city_business\n",
      "- product_C\n",
      "- product_E\n",
      "- time_of_day_afternoon\n",
      "- hour_bin_afternoon\n",
      "- hour_bin_evening\n",
      "- campaign_hour_bin_105960.0_early_morning\n",
      "- campaign_hour_bin_105960.0_morning\n",
      "- campaign_hour_bin_396664.0_evening\n",
      "- campaign_hour_bin_396664.0_night\n",
      "- campaign_hour_bin_404347.0_night\n",
      "- campaign_hour_bin_82320.0_afternoon\n",
      "- campaign_hour_bin_82320.0_morning\n",
      "- campaign_hour_bin_98970.0_afternoon\n",
      "- user_depth_time_2.0_morning\n",
      "- user_depth_time_3.0_afternoon\n",
      "- user_depth_time_3.0_evening\n",
      "- user_depth_time_3.0_night\n",
      "- gender_age_Female_2.0\n",
      "- gender_age_Female_3.0\n",
      "\n",
      "Final feature set: 91 features\n",
      "\n",
      "Remaining features:\n",
      "- campaign_id\n",
      "- webpage_id\n",
      "- product_category_1\n",
      "- user_group_id\n",
      "- age_level\n",
      "- city_development_index\n",
      "- var_1\n",
      "- product_category_2_missing\n",
      "- hour\n",
      "- day_of_week\n",
      "- is_weekend\n",
      "- is_early_morning\n",
      "- is_holiday\n",
      "- is_near_holiday\n",
      "- historical_user_ctr\n",
      "- session_count_log\n",
      "- sessions_per_day_mean\n",
      "- sessions_per_day_mean_log\n",
      "- time_since_last_click\n",
      "- click_frequency_24h\n",
      "- click_frequency_24h_log\n",
      "- historical_user_ctr_rank\n",
      "- campaign_historical_ctr_log\n",
      "- campaign_success_percentile\n",
      "- campaign_early_morning\n",
      "- age_weekend\n",
      "- user_depth_age\n",
      "- gender_Male\n",
      "- product_B\n",
      "- product_D\n",
      "- product_F\n",
      "- product_G\n",
      "- product_H\n",
      "- product_I\n",
      "- product_J\n",
      "- time_of_day_morning\n",
      "- time_of_day_evening\n",
      "- hour_bin_early_morning\n",
      "- hour_bin_morning\n",
      "- hour_bin_night\n",
      "- session_count_bin_L\n",
      "- session_count_bin_M\n",
      "- campaign_hour_bin_118601.0_afternoon\n",
      "- campaign_hour_bin_118601.0_early_morning\n",
      "- campaign_hour_bin_118601.0_evening\n",
      "- campaign_hour_bin_118601.0_morning\n",
      "- campaign_hour_bin_118601.0_night\n",
      "- campaign_hour_bin_359520.0_afternoon\n",
      "- campaign_hour_bin_359520.0_early_morning\n",
      "- campaign_hour_bin_359520.0_evening\n",
      "- campaign_hour_bin_359520.0_morning\n",
      "- campaign_hour_bin_359520.0_night\n",
      "- campaign_hour_bin_360936.0_afternoon\n",
      "- campaign_hour_bin_360936.0_early_morning\n",
      "- campaign_hour_bin_360936.0_evening\n",
      "- campaign_hour_bin_360936.0_morning\n",
      "- campaign_hour_bin_360936.0_night\n",
      "- campaign_hour_bin_404347.0_afternoon\n",
      "- campaign_hour_bin_404347.0_evening\n",
      "- campaign_hour_bin_404347.0_morning\n",
      "- campaign_hour_bin_405490.0_afternoon\n",
      "- campaign_hour_bin_405490.0_early_morning\n",
      "- campaign_hour_bin_405490.0_evening\n",
      "- campaign_hour_bin_405490.0_morning\n",
      "- campaign_hour_bin_405490.0_night\n",
      "- campaign_hour_bin_414149.0_afternoon\n",
      "- campaign_hour_bin_414149.0_evening\n",
      "- campaign_hour_bin_414149.0_morning\n",
      "- campaign_hour_bin_82320.0_evening\n",
      "- campaign_hour_bin_82320.0_night\n",
      "- campaign_hour_bin_98970.0_evening\n",
      "- campaign_hour_bin_98970.0_morning\n",
      "- campaign_hour_bin_98970.0_night\n",
      "- user_depth_time_2.0_afternoon\n",
      "- user_depth_time_2.0_evening\n",
      "- user_depth_time_3.0_morning\n",
      "- user_sessions_time_L_afternoon\n",
      "- user_sessions_time_L_evening\n",
      "- user_sessions_time_L_morning\n",
      "- user_sessions_time_L_night\n",
      "- user_sessions_time_VL_afternoon\n",
      "- user_sessions_time_VL_evening\n",
      "- user_sessions_time_VL_morning\n",
      "- user_sessions_time_VL_night\n",
      "- gender_age_Female_4.0\n",
      "- gender_age_Female_5.0\n",
      "- gender_age_Male_1.0\n",
      "- gender_age_Male_2.0\n",
      "- gender_age_Male_3.0\n",
      "- gender_age_Male_4.0\n",
      "- gender_age_Male_5.0\n"
     ]
    }
   ],
   "source": [
    "def prefilter_features(df: pd.DataFrame, target_col: str = 'is_click', verbose: bool = True):\n",
    "    \"\"\"\n",
    "    Multi-stage feature pre-filtering process.\n",
    "    Returns a list of features that pass all filtering stages.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial_features = len(df.columns) - 1  # exclude target\n",
    "    if verbose:\n",
    "        print(f\"Starting with {initial_features} features\")\n",
    "    \n",
    "    # remove constant and quasi-constant features\n",
    "    def remove_low_variance_features(df, threshold=0.01):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        variances = df[numeric_cols].var()\n",
    "        low_var_features = variances[variances < threshold].index.tolist()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nLow variance features removed ({len(low_var_features)}):\")\n",
    "            for f in low_var_features:\n",
    "                print(f\"- {f} (variance: {variances[f]:.6f})\")\n",
    "        \n",
    "        return df.drop(columns=low_var_features)\n",
    "\n",
    "    # remove highly correlated features\n",
    "    def remove_correlated_features(df, threshold=0.95):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        corr_matrix = df[numeric_cols].corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        \n",
    "        high_corr_features = []\n",
    "        seen_pairs = set()\n",
    "        \n",
    "        # find features to remove\n",
    "        for col in upper.columns:\n",
    "            # get correlations above threshold\n",
    "            high_corr = upper[col][upper[col] > threshold].index.tolist()\n",
    "            for feat in high_corr:\n",
    "                if (col, feat) not in seen_pairs and (feat, col) not in seen_pairs:\n",
    "                    # keep the one with higher correlation with target\n",
    "                    corr_with_target = abs(df[[col, feat, target_col]].corr()[target_col])\n",
    "                    if corr_with_target[col] < corr_with_target[feat]:\n",
    "                        high_corr_features.append(col)\n",
    "                    else:\n",
    "                        high_corr_features.append(feat)\n",
    "                    seen_pairs.add((col, feat))\n",
    "        \n",
    "        high_corr_features = list(set(high_corr_features))\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nHighly correlated features removed ({len(high_corr_features)}):\")\n",
    "            for f in high_corr_features:\n",
    "                print(f\"- {f}\")\n",
    "        \n",
    "        return df.drop(columns=high_corr_features)\n",
    "\n",
    "    # remove features with low mutual information\n",
    "    def remove_low_mi_features(df, target_col, threshold=0.001):\n",
    "        # separate numeric and categorical columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_cols.remove(target_col)\n",
    "        \n",
    "        # calculate MI scores for numeric features\n",
    "        mi_scores = mutual_info_classif(\n",
    "            StandardScaler().fit_transform(df[numeric_cols]), \n",
    "            df[target_col],\n",
    "            random_state=42\n",
    "        )\n",
    "        mi_series = pd.Series(mi_scores, index=numeric_cols)\n",
    "        \n",
    "        # identify features with low MI scores\n",
    "        low_mi_features = mi_series[mi_series < threshold].index.tolist()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nLow mutual information features removed ({len(low_mi_features)}):\")\n",
    "            for f in low_mi_features:\n",
    "                print(f\"- {f} (MI score: {mi_series[f]:.6f})\")\n",
    "        \n",
    "        return df.drop(columns=low_mi_features)\n",
    "\n",
    "\n",
    "    # remove features with high p-value in univariate testing\n",
    "    def remove_insignificant_features(df, target_col, p_threshold=0.05):\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_cols.remove(target_col)\n",
    "        \n",
    "        insignificant_features = []\n",
    "        for col in numeric_cols:\n",
    "            _, p_value = stats.mannwhitneyu(\n",
    "                df[df[target_col] == 1][col],\n",
    "                df[df[target_col] == 0][col],\n",
    "                alternative='two-sided'\n",
    "            )\n",
    "            if p_value > p_threshold:\n",
    "                insignificant_features.append(col)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nStatistically insignificant features removed ({len(insignificant_features)}):\")\n",
    "            for f in insignificant_features:\n",
    "                print(f\"- {f}\")\n",
    "        \n",
    "        return df.drop(columns=insignificant_features)\n",
    "\n",
    "\n",
    "    # apply all filtering stages\n",
    "    df = remove_low_variance_features(df)\n",
    "    df = remove_correlated_features(df)\n",
    "    df = remove_low_mi_features(df, target_col)\n",
    "    df = remove_insignificant_features(df, target_col)\n",
    "    \n",
    "    selected_features = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nFinal feature set: {len(selected_features)} features\")\n",
    "        print(\"\\nRemaining features:\")\n",
    "        for f in selected_features:\n",
    "            print(f\"- {f}\")\n",
    "    \n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# run through the pre-filtering process to see its suggestions\n",
    "selected_features = prefilter_features(df_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all of the non-selected features from the dataframe\n",
    "X = df_prepared[selected_features]\n",
    "y = df_prepared['is_click']\n",
    "\n",
    "if 'is_click' in X.columns:\n",
    "    X = X.drop('is_click', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# remove all of the non-selected features from the lists: \"categorical_features\", \"features_to_scale\", and \"keep_as_is\"\n",
    "categorical_features = [col for col in categorical_features if col in selected_features]\n",
    "features_to_scale = [col for col in features_to_scale if col in selected_features]\n",
    "keep_as_is = [col for col in keep_as_is if col in selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the features that need it, by training the scaler on the training set\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = X_train.copy()\n",
    "X_test_scaled = X_test.copy()\n",
    "\n",
    "X_train_scaled[features_to_scale] = scaler.fit_transform(X_train[features_to_scale])\n",
    "X_test_scaled[features_to_scale] = scaler.transform(X_test[features_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feature_importance_optimized(X, y, n_bootstraps=100, sample_frac=0.8):\n",
    "    \"\"\"\n",
    "    Evaluate feature importance using bootstrapped samples with debug prints.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize score arrays\n",
    "    mi_scores = np.zeros((n_bootstraps, X.shape[1]), dtype=np.float32)\n",
    "    rf_importance = np.zeros((n_bootstraps, X.shape[1]), dtype=np.float32)\n",
    "    \n",
    "    # combine features and target for sampling\n",
    "    data = pd.concat([X, y.rename('target')], axis=1)\n",
    "    \n",
    "    # initialize random forest\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for i in range(n_bootstraps):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"\\nBootstrap iteration {i}/{n_bootstraps}\")\n",
    "        \n",
    "        try:\n",
    "            bootstrap_sample = data.sample(frac=sample_frac, replace=True, random_state=i)\n",
    "            \n",
    "            if i == 0:  # detailed diagnostics for first iteration\n",
    "                print(f\"Bootstrap {i} details:\")\n",
    "                print(f\"Sample shape: {bootstrap_sample.shape}\")\n",
    "                print(f\"Sample NaN in features: {bootstrap_sample.drop('target', axis=1).isna().sum().sum()}\")\n",
    "                print(f\"Sample NaN in target: {bootstrap_sample['target'].isna().sum()}\")\n",
    "            \n",
    "            X_boot = bootstrap_sample.drop('target', axis=1)\n",
    "            y_boot = bootstrap_sample['target']\n",
    "            \n",
    "            # check for NaN values before MI calculation\n",
    "            if X_boot.isna().any().any() or y_boot.isna().any():\n",
    "                print(f\"Bootstrap {i} has NaN values:\")\n",
    "                print(f\"X_boot NaN count: {X_boot.isna().sum().sum()}\")\n",
    "                print(f\"y_boot NaN count: {y_boot.isna().sum()}\")\n",
    "                raise ValueError(\"NaN values detected in bootstrap sample\")\n",
    "            \n",
    "            # compute mutual information scores\n",
    "            mi_scores[i] = mutual_info_classif(X_boot, y_boot, random_state=42)\n",
    "            \n",
    "            # fit random forest and get feature importances\n",
    "            rf.fit(X_boot, y_boot)\n",
    "            rf_importance[i] = rf.feature_importances_\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in bootstrap {i}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    results = []\n",
    "    for j, feature in enumerate(X.columns):\n",
    "        valid_mi = mi_scores[:, j][~np.isnan(mi_scores[:, j])]\n",
    "        valid_rf = rf_importance[:, j][~np.isnan(rf_importance[:, j])]\n",
    "        \n",
    "        if len(valid_mi) > 0 and len(valid_rf) > 0:\n",
    "            mi_mean = valid_mi.mean()\n",
    "            mi_std = valid_mi.std()\n",
    "            mi_ci = stats.norm.interval(0.95, mi_mean, mi_std)\n",
    "            \n",
    "            rf_mean = valid_rf.mean()\n",
    "            rf_std = valid_rf.std()\n",
    "            rf_ci = stats.norm.interval(0.95, rf_mean, rf_std)\n",
    "            \n",
    "            results.append({\n",
    "                'feature': feature,\n",
    "                'mi_score': mi_mean,\n",
    "                'mi_ci_lower': mi_ci[0],\n",
    "                'mi_ci_upper': mi_ci[1],\n",
    "                'rf_importance': rf_mean,\n",
    "                'rf_ci_lower': rf_ci[0],\n",
    "                'rf_ci_upper': rf_ci[1]\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def select_features_cv_optimized(X, y, base_features, threshold=0.01):\n",
    "    \"\"\"\n",
    "    Perform forward feature selection using cross-validation.\n",
    "    \"\"\"\n",
    "    print(f\"\\nStarting CV selection with {len(base_features)} base features\")\n",
    "    \n",
    "    selected_features = base_features.copy()\n",
    "    remaining_features = [f for f in X.columns if f not in selected_features]\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    estimator = RandomForestClassifier(\n",
    "        n_estimators=30,\n",
    "        max_depth=10,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        base_score = cross_val_score(\n",
    "            estimator,\n",
    "            X[selected_features],\n",
    "            y,\n",
    "            cv=cv,\n",
    "            scoring='f1'\n",
    "        ).mean()\n",
    "        \n",
    "        print(f\"Base CV score: {base_score:.4f}\")\n",
    "        \n",
    "        improved = True\n",
    "        while improved and remaining_features:\n",
    "            improved = False\n",
    "            scores = {}\n",
    "            \n",
    "            for feature in remaining_features:\n",
    "                current_features = selected_features + [feature]\n",
    "                try:\n",
    "                    score = cross_val_score(\n",
    "                        estimator,\n",
    "                        X[current_features],\n",
    "                        y,\n",
    "                        cv=cv,\n",
    "                        scoring='f1',\n",
    "                        n_jobs=-1\n",
    "                    ).mean()\n",
    "                    scores[feature] = score\n",
    "                except Exception as e:\n",
    "                    print(f\"Error evaluating feature {feature}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if scores:\n",
    "                best_feature, best_score = max(scores.items(), key=lambda x: x[1])\n",
    "                if best_score > base_score + threshold:\n",
    "                    selected_features.append(best_feature)\n",
    "                    remaining_features.remove(best_feature)\n",
    "                    base_score = best_score\n",
    "                    print(f\"Added {best_feature} (new f1: {best_score:.4f})\")\n",
    "                    improved = True\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in CV selection: {str(e)}\")\n",
    "        return base_features\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def select_features_optimized(X_train_scaled, y_train, X_test_scaled=None):\n",
    "    \"\"\"\n",
    "    Main feature selection function combining importance evaluation and CV selection.\n",
    "    \"\"\"\n",
    "    print(\"Starting feature selection process...\")\n",
    "    print(f\"Input shape: {X_train_scaled.shape}\")\n",
    "\n",
    "    try:\n",
    "        # evaluate feature importance with bootstrapping\n",
    "        importance_df = evaluate_feature_importance_optimized(\n",
    "            X_train_scaled,\n",
    "            y_train,\n",
    "            n_bootstraps=100,\n",
    "            sample_frac=0.8\n",
    "        )\n",
    "        \n",
    "        print(\"\\nFeature importance evaluation complete.\")\n",
    "        print(f\"Evaluated {len(importance_df)} features\")\n",
    "\n",
    "        # identify stable features (positive confidence intervals)\n",
    "        stable_features = importance_df[\n",
    "            (importance_df['mi_ci_lower'] > 0) &\n",
    "            (importance_df['rf_ci_lower'] > 0)\n",
    "        ]['feature'].tolist()\n",
    "\n",
    "        print(f\"\\nFound {len(stable_features)} stable features\")\n",
    "\n",
    "        # select base features (above median importance)\n",
    "        base_features = importance_df[\n",
    "            (importance_df['mi_score'] > importance_df['mi_score'].median()) &\n",
    "            (importance_df['rf_importance'] > importance_df['rf_importance'].median())\n",
    "        ]['feature'].tolist()\n",
    "\n",
    "        # perform forward selection on stable features\n",
    "        selected_features = select_features_cv_optimized(\n",
    "            X_train_scaled[stable_features],\n",
    "            y_train,\n",
    "            base_features\n",
    "        )\n",
    "        print(\"\\nFeature Selection Summary:\")\n",
    "        print(f\"Initial features: {X_train_scaled.shape[1]}\")\n",
    "        print(f\"Stable features: {len(stable_features)}\")\n",
    "        print(f\"Final selected features: {len(selected_features)}\")\n",
    "\n",
    "        return selected_features, importance_df\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main feature selection: {str(e)}\")\n",
    "        print(\"Falling back to all features\")\n",
    "        return list(X_train_scaled.columns), pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature selection process...\n",
      "Input shape: (295721, 91)\n",
      "\n",
      "Initial Data Diagnostics:\n",
      "X shape: (295721, 91)\n",
      "y shape: (295721,)\n",
      "X NaN count:\n",
      "0\n",
      "y NaN count: 0\n",
      "\n",
      "Data Types:\n",
      "X dtypes:\n",
      "float64    82\n",
      "int64       5\n",
      "int8        2\n",
      "int32       2\n",
      "Name: count, dtype: int64\n",
      "y dtype: float64\n",
      "\n",
      "First few column names:\n",
      "['campaign_id', 'webpage_id', 'product_category_1', 'user_group_id', 'age_level']\n",
      "\n",
      "Combining X and y:\n",
      "Combined data shape: (295721, 92)\n",
      "Combined data NaN count: 0\n",
      "\n",
      "Testing single bootstrap sample:\n",
      "Sample shape: (236577, 92)\n",
      "Sample NaN count: 0\n",
      "Sample target NaN count: 0\n",
      "\n",
      "Bootstrap iteration 0/100\n",
      "Bootstrap 0 details:\n",
      "Sample shape: (236577, 92)\n",
      "Sample NaN in features: 0\n",
      "Sample NaN in target: 0\n",
      "\n",
      "Bootstrap iteration 10/100\n",
      "\n",
      "Bootstrap iteration 20/100\n",
      "\n",
      "Bootstrap iteration 30/100\n",
      "\n",
      "Bootstrap iteration 40/100\n",
      "\n",
      "Bootstrap iteration 50/100\n",
      "\n",
      "Bootstrap iteration 60/100\n",
      "\n",
      "Bootstrap iteration 70/100\n",
      "\n",
      "Bootstrap iteration 80/100\n",
      "\n",
      "Bootstrap iteration 90/100\n",
      "\n",
      "Feature importance evaluation complete.\n",
      "Evaluated 91 features\n",
      "\n",
      "Found 50 stable features\n",
      "\n",
      "Starting CV selection with 40 base features\n",
      "Base CV score: 0.1071\n",
      "\n",
      "Feature Selection Summary:\n",
      "Initial features: 91\n",
      "Stable features: 50\n",
      "Final selected features: 40\n",
      "\n",
      "Selected Features:\n",
      "- campaign_id\n",
      "- webpage_id\n",
      "- product_category_1\n",
      "- user_group_id\n",
      "- age_level\n",
      "- city_development_index\n",
      "- var_1\n",
      "- product_category_2_missing\n",
      "- hour\n",
      "- day_of_week\n",
      "- is_weekend\n",
      "- is_holiday\n",
      "- is_near_holiday\n",
      "- historical_user_ctr\n",
      "- session_count_log\n",
      "- sessions_per_day_mean\n",
      "- sessions_per_day_mean_log\n",
      "- time_since_last_click\n",
      "- click_frequency_24h\n",
      "- click_frequency_24h_log\n",
      "- historical_user_ctr_rank\n",
      "- campaign_historical_ctr_log\n",
      "- campaign_success_percentile\n",
      "- campaign_early_morning\n",
      "- age_weekend\n",
      "- user_depth_age\n",
      "- gender_Male\n",
      "- product_H\n",
      "- product_I\n",
      "- time_of_day_morning\n",
      "- hour_bin_early_morning\n",
      "- hour_bin_morning\n",
      "- session_count_bin_L\n",
      "- user_depth_time_3.0_morning\n",
      "- user_sessions_time_VL_afternoon\n",
      "- user_sessions_time_VL_morning\n",
      "- user_sessions_time_VL_night\n",
      "- gender_age_Male_2.0\n",
      "- gender_age_Male_3.0\n",
      "- gender_age_Male_4.0\n",
      "\n",
      "Top 10 Features by Importance:\n",
      "                        feature  mi_score  mi_ci_lower  mi_ci_upper  \\\n",
      "23  campaign_success_percentile  0.021537     0.020168     0.022906   \n",
      "22  campaign_historical_ctr_log  0.041832     0.039850     0.043814   \n",
      "14          historical_user_ctr  0.014813     0.013913     0.015713   \n",
      "8                          hour  0.002507     0.001513     0.003502   \n",
      "21     historical_user_ctr_rank  0.011110     0.010152     0.012067   \n",
      "18        time_since_last_click  0.017877     0.016653     0.019102   \n",
      "16        sessions_per_day_mean  0.004403     0.003463     0.005343   \n",
      "15            session_count_log  0.008402     0.007382     0.009421   \n",
      "17    sessions_per_day_mean_log  0.010614     0.009595     0.011633   \n",
      "0                   campaign_id  0.008560     0.007698     0.009421   \n",
      "\n",
      "    rf_importance  rf_ci_lower  rf_ci_upper  \n",
      "23       0.465671     0.430695     0.500647  \n",
      "22       0.070087     0.060947     0.079226  \n",
      "14       0.047185     0.039921     0.054450  \n",
      "8        0.031460     0.025283     0.037637  \n",
      "21       0.026219     0.022235     0.030203  \n",
      "18       0.025871     0.022209     0.029533  \n",
      "16       0.023034     0.019270     0.026799  \n",
      "15       0.023034     0.018609     0.027459  \n",
      "17       0.021989     0.017920     0.026058  \n",
      "0        0.016695     0.012580     0.020810  \n"
     ]
    }
   ],
   "source": [
    "# call the optimized feature selection function\n",
    "final_selected_features, importance_df = select_features_optimized(X_train_scaled, y_train, X_test_scaled)\n",
    "\n",
    "# review results\n",
    "print(\"\\nSelected Features:\")\n",
    "for f in final_selected_features:\n",
    "    print(f\"- {f}\")\n",
    "\n",
    "print(\"\\nTop 10 Features by Importance:\")\n",
    "print(importance_df.sort_values(by='rf_importance', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only selected features in train and test sets\n",
    "X_train_final_latest = X_train_scaled[final_selected_features]\n",
    "X_test_final_latest = X_test_scaled[final_selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression Results:\n",
      "--------------------------------------------------\n",
      "F1 Score: 0.1388\n",
      "Optimal Threshold: 0.55\n",
      "Cross-validation F1: nan (+/- nan)\n",
      "\n",
      "Additional Metrics:\n",
      "ROC AUC Score: 0.5690\n",
      "Average Precision Score: 0.0868\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.80      0.86     68969\n",
      "         1.0       0.09      0.28      0.14      4962\n",
      "\n",
      "    accuracy                           0.76     73931\n",
      "   macro avg       0.52      0.54      0.50     73931\n",
      "weighted avg       0.88      0.76      0.81     73931\n",
      "\n",
      "\n",
      "Random Forest Results:\n",
      "--------------------------------------------------\n",
      "F1 Score: 0.2615\n",
      "Optimal Threshold: 0.60\n",
      "Cross-validation F1: nan (+/- nan)\n",
      "\n",
      "Additional Metrics:\n",
      "ROC AUC Score: 0.7144\n",
      "Average Precision Score: 0.2384\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.96      0.95     68969\n",
      "         1.0       0.29      0.24      0.26      4962\n",
      "\n",
      "    accuracy                           0.91     73931\n",
      "   macro avg       0.62      0.60      0.61     73931\n",
      "weighted avg       0.90      0.91      0.91     73931\n",
      "\n",
      "\n",
      "Gradient Boosting Results:\n",
      "--------------------------------------------------\n",
      "F1 Score: 0.2612\n",
      "Optimal Threshold: 0.20\n",
      "Cross-validation F1: nan (+/- nan)\n",
      "\n",
      "Additional Metrics:\n",
      "ROC AUC Score: 0.7517\n",
      "Average Precision Score: 0.2567\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.98      0.96     68969\n",
      "         1.0       0.42      0.19      0.26      4962\n",
      "\n",
      "    accuracy                           0.93     73931\n",
      "   macro avg       0.68      0.59      0.61     73931\n",
      "weighted avg       0.91      0.93      0.91     73931\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # find optimal threshold for F1 score\n",
    "    thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "    f1_scores = []\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba > threshold).astype(int)\n",
    "        f1_scores.append(f1_score(y_test, y_pred))\n",
    "    \n",
    "    optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "    y_pred = (y_pred_proba > optimal_threshold).astype(int)\n",
    "    \n",
    "    # CV with custom F1 scoring\n",
    "    def f1_score_with_threshold(estimator, X, y):\n",
    "        probs = estimator.predict_proba(X)[:, 1]\n",
    "        preds = (probs > optimal_threshold).astype(int)\n",
    "        return f1_score(y, preds)\n",
    "    \n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, \n",
    "        cv=5, \n",
    "        scoring=make_scorer(f1_score_with_threshold)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"F1 Score: {f1_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Optimal Threshold: {optimal_threshold:.2f}\")\n",
    "    print(f\"Cross-validation F1: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(\"\\nAdditional Metrics:\")\n",
    "    print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(f\"Average Precision Score: {average_precision_score(y_test, y_pred_proba):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return model, optimal_threshold\n",
    "\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = evaluate_model(\n",
    "    LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    X_train_final_latest, X_test_final_latest, y_train, y_test,\n",
    "    \"Logistic Regression\"\n",
    ")\n",
    "\n",
    "# Random Forest\n",
    "rf_model = evaluate_model(\n",
    "    RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    X_train_final_latest, X_test_final_latest, y_train, y_test,\n",
    "    \"Random Forest\"\n",
    ")\n",
    "\n",
    "# Gradient Boosting\n",
    "gb_model = evaluate_model(\n",
    "    GradientBoostingClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.1,\n",
    "        random_state=42\n",
    "    ),\n",
    "    X_train_final_latest, X_test_final_latest, y_train, y_test,\n",
    "    \"Gradient Boosting\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.67459\ttrain-auc:0.74801\ttest-logloss:0.67540\ttest-auc:0.71910\n",
      "[50]\ttrain-logloss:0.50333\ttrain-auc:0.85000\ttest-logloss:0.52841\ttest-auc:0.74273\n",
      "[66]\ttrain-logloss:0.48152\ttrain-auc:0.87289\ttest-logloss:0.51285\ttest-auc:0.74145\n"
     ]
    }
   ],
   "source": [
    "dtrain = xgb.DMatrix(X_train_final_latest, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test_final_latest, label=y_test)\n",
    "\n",
    "params = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.1,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': ['logloss', 'auc'],\n",
    "    'seed': 42,\n",
    "    'min_child_weight': 3,  # Helps prevent overfitting\n",
    "    'subsample': 0.9,      # Slight randomness\n",
    "    'colsample_bytree': 0.9,  # Slight feature sampling\n",
    "    'scale_pos_weight': 68969/4962  # Balance of positive and negative weights\n",
    "}\n",
    "\n",
    "\n",
    "# xgboost model\n",
    "xgb_model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=200,\n",
    "    evals=[(dtrain, 'train'), (dtest, 'test')],\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# cross-validation\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    nfold=5,\n",
    "    early_stopping_rounds=20,\n",
    "    metrics=['auc', 'logloss'],\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Results:\n",
      "--------------------------------------------------\n",
      "ROC AUC Score: 0.7414462823254995\n",
      "F1 Score: 0.2750130958617077\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.95      0.93      0.94     68969\n",
      "         1.0       0.24      0.32      0.28      4962\n",
      "\n",
      "    accuracy                           0.89     73931\n",
      "   macro avg       0.60      0.62      0.61     73931\n",
      "weighted avg       0.90      0.89      0.89     73931\n",
      "\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "                        feature  importance\n",
      "22  campaign_success_percentile  107.631897\n",
      "1                    webpage_id   87.769150\n",
      "13          historical_user_ctr   67.916389\n",
      "0                   campaign_id   60.535324\n",
      "21  campaign_historical_ctr_log   40.709793\n",
      "11                   is_holiday   35.754414\n",
      "24                  age_weekend   35.452583\n",
      "7    product_category_2_missing   33.822784\n",
      "9                   day_of_week   33.422920\n",
      "30       hour_bin_early_morning   33.307892\n"
     ]
    }
   ],
   "source": [
    "y_pred_proba = xgb_model.predict(dtest)  # predict probabilities\n",
    "\n",
    "# find the optimal threshold for F1 score\n",
    "thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "f1_scores = []\n",
    "for threshold in thresholds:\n",
    "    y_pred_thresh = (y_pred_proba > threshold).astype(int)\n",
    "    f1_scores.append(f1_score(y_test, y_pred_thresh))\n",
    "\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "y_pred_optimal = (y_pred_proba > optimal_threshold).astype(int)\n",
    "\n",
    "y_pred = (y_pred_proba > optimal_threshold).astype(int)\n",
    "\n",
    "# feature importance\n",
    "importance_dict = xgb_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame(\n",
    "    [(k, v) for k, v in importance_dict.items()],\n",
    "    columns=['feature', 'importance']\n",
    ").sort_values('importance', ascending=False)\n",
    "\n",
    "# print evaluation metrics\n",
    "print(\"\\nXGBoost Results:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(importance_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting grid search...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:   6%|▋         | 1/16 [01:18<19:41, 78.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2201\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 3, 'eta': 0.1, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  12%|█▎        | 2/16 [02:36<18:11, 77.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2484\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 3, 'eta': 0.1, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  38%|███▊      | 6/16 [07:16<11:43, 70.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2594\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 3, 'eta': 0.2, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  62%|██████▎   | 10/16 [12:01<07:28, 74.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2610\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 5, 'eta': 0.1, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  75%|███████▌  | 12/16 [14:42<05:10, 77.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2627\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 5, 'eta': 0.1, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress:  88%|████████▊ | 14/16 [17:22<02:37, 78.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2668\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 5, 'eta': 0.2, 'min_child_weight': 1, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grid Search Progress: 100%|██████████| 16/16 [20:03<00:00, 75.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New best F1 score: 0.2677\n",
      "Parameters: {'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 5, 'eta': 0.2, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n",
      "\n",
      "Best parameters found:\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'logloss', 'seed': 42, 'max_depth': 5, 'eta': 0.2, 'min_child_weight': 3, 'subsample': 0.9, 'colsample_bytree': 0.9, 'scale_pos_weight': 10}\n",
      "Best CV F1 score: 0.2677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# define our training and validation data as DMatrix objects\n",
    "dtrain = xgb.DMatrix(X_train_final_latest, label=y_train)\n",
    "dval = xgb.DMatrix(X_test_final_latest, label=y_test)\n",
    "\n",
    "# define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5],           \n",
    "    'eta': [0.1, 0.2],            \n",
    "    'min_child_weight': [1, 3],    \n",
    "    'subsample': [0.9],            \n",
    "    'colsample_bytree': [0.9],     \n",
    "    'scale_pos_weight': [5, 10]    \n",
    "}\n",
    "\n",
    "# function to calculate F1 score for XGBoost predictions\n",
    "def f1_score_xgb(predt: np.ndarray, dtrain: xgb.DMatrix):\n",
    "    y_true = dtrain.get_label()\n",
    "    thresholds = np.arange(0.2, 0.8, 0.05)\n",
    "    best_f1, best_threshold = 0, None\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (predt > threshold).astype(int)\n",
    "        # explicitly specify pos_label=1 to focus on click predictions\n",
    "        f1 = f1_score(y_true, y_pred, pos_label=1)\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    global optimal_threshold\n",
    "    optimal_threshold = best_threshold\n",
    "    \n",
    "    return 'f1', best_f1\n",
    "\n",
    "\n",
    "# function to perform k-fold cross validation for a set of parameters\n",
    "def xgb_cv_score(params, dtrain, num_boost_round=50, nfold=2):\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=num_boost_round,\n",
    "        nfold=nfold,\n",
    "        feval=f1_score_xgb,\n",
    "        maximize=True,\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    return cv_results['test-f1-mean'].max()\n",
    "\n",
    "\n",
    "# initialize best parameters and score\n",
    "best_params = None\n",
    "best_score = 0\n",
    "\n",
    "# base, constant parameters\n",
    "base_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "\n",
    "# Calculate total number of combinations for tqdm\n",
    "param_combinations = list(product(\n",
    "    param_grid['max_depth'],\n",
    "    param_grid['eta'],\n",
    "    param_grid['min_child_weight'],\n",
    "    param_grid['subsample'],\n",
    "    param_grid['colsample_bytree'],\n",
    "    param_grid['scale_pos_weight']\n",
    "))\n",
    "\n",
    "# manual grid search with tqdm\n",
    "for max_depth, eta, min_child_weight, subsample, colsample_bytree, scale_pos_weight in tqdm(param_combinations, desc=\"Grid Search Progress\"):\n",
    "\n",
    "    params = {\n",
    "        **base_params,\n",
    "        'max_depth': max_depth,\n",
    "        'eta': eta,\n",
    "        'min_child_weight': min_child_weight,\n",
    "        'subsample': subsample,\n",
    "        'colsample_bytree': colsample_bytree,\n",
    "        'scale_pos_weight': scale_pos_weight\n",
    "    }\n",
    "    \n",
    "    score = xgb_cv_score(params, dtrain)\n",
    "    \n",
    "    # update best parameters if score is better\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_params = params.copy()\n",
    "        print(f\"\\nNew best F1 score: {best_score:.4f}\")\n",
    "        print(\"Parameters:\", best_params)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(best_params)\n",
    "print(f\"Best CV F1 score: {best_score:.4f}\")\n",
    "\n",
    "\n",
    "# train final model with best parameters\n",
    "final_model = xgb.train(\n",
    "    best_params,\n",
    "    dtrain,\n",
    "    num_boost_round=100,\n",
    "    evals=[(dtrain, 'train'), (dval, 'val')],\n",
    "    feval=f1_score_xgb,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose_eval=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Data Metrics:\n",
      "--------------------------------------------------\n",
      "Positive rate (CTR): 6.7124%\n",
      "Class imbalance ratio: 13.90:1\n",
      "Optimal threshold: 0.2500\n",
      "Best F1 score: 0.2750\n",
      "\n",
      "Probability Distributions:\n",
      "--------------------------------------------------\n",
      "Training set:\n",
      "min: 0.0028\n",
      "max: 0.9893\n",
      "mean: 0.3672\n",
      "std: 0.2107\n",
      "\n",
      "Validation set:\n",
      "min: 0.0043\n",
      "max: 0.9882\n",
      "mean: 0.3685\n",
      "std: 0.2080\n"
     ]
    }
   ],
   "source": [
    "# calculate training metrics\n",
    "training_metrics = {\n",
    "    'positive_rate': (y_train == 1).mean(),\n",
    "    'probability_distribution': {\n",
    "        'train_probabilities': xgb_model.predict(dtrain),\n",
    "        'val_probabilities': xgb_model.predict(dval)\n",
    "    },\n",
    "    'best_f1': max(f1_scores),\n",
    "    'optimal_threshold': optimal_threshold,\n",
    "    'class_balance': len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "}\n",
    "\n",
    "# probability distribution stats\n",
    "for data_split in ['train_probabilities', 'val_probabilities']:\n",
    "    probs = training_metrics['probability_distribution'][data_split]\n",
    "    training_metrics['probability_distribution'][f'{data_split}_stats'] = {\n",
    "        'min': probs.min(),\n",
    "        'max': probs.max(),\n",
    "        'mean': probs.mean(),\n",
    "        'std': probs.std()\n",
    "    }\n",
    "\n",
    "# metrics summary\n",
    "print(\"\\nTraining Data Metrics:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Positive rate (CTR): {training_metrics['positive_rate']:.4%}\")\n",
    "print(f\"Class imbalance ratio: {training_metrics['class_balance']:.2f}:1\")\n",
    "print(f\"Optimal threshold: {training_metrics['optimal_threshold']:.4f}\")\n",
    "print(f\"Best F1 score: {training_metrics['best_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nProbability Distributions:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Training set:\")\n",
    "for k, v in training_metrics['probability_distribution']['train_probabilities_stats'].items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "print(\"\\nValidation set:\")\n",
    "for k, v in training_metrics['probability_distribution']['val_probabilities_stats'].items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the Test Batch for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60228, 14)\n"
     ]
    }
   ],
   "source": [
    "# load the test data\n",
    "test_data = pd.read_csv('X_test_1st.csv')\n",
    "\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final_model.joblib']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saveing both the scaler and model:\n",
    "joblib.dump(scaler, 'feature_scaler.joblib')\n",
    "joblib.dump(final_model, 'final_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_empirical_defaults(df):\n",
    "    \"\"\"\n",
    "    Calculate empirical default values from training data.\n",
    "    \"\"\"\n",
    "    engagement_defaults = {\n",
    "        \"historical_user_ctr\": df['historical_user_ctr'].mean(),\n",
    "        \"session_count_log\": df['session_count_log'].median(),\n",
    "        \"sessions_per_day_mean\": df['sessions_per_day_mean'].mean(),\n",
    "        \"sessions_per_day_mean_log\": df['sessions_per_day_mean_log'].median(),\n",
    "        \"time_since_last_click\": df['time_since_last_click'].median(),\n",
    "        \"click_frequency_24h\": df['click_frequency_24h'].mean(),\n",
    "        \"click_frequency_24h_log\": df['click_frequency_24h_log'].median(),\n",
    "        \"historical_user_ctr_rank\": 0.5  # Use middle rank for new users\n",
    "    }\n",
    "    \n",
    "    # convert campaign-hour tuples to strings for JSON serialization\n",
    "    campaign_hour_perf = df.groupby(['campaign_id', 'hour'])['campaign_hour_relative'].mean()\n",
    "    campaign_hour_dict = {\n",
    "        f\"{campaign_id}_{hour}\": value \n",
    "        for (campaign_id, hour), value in campaign_hour_perf.items()\n",
    "    }\n",
    "    \n",
    "    campaign_defaults = {\n",
    "        # global campaign metrics\n",
    "        \"campaign_historical_ctr_log\": df.groupby('campaign_id')['campaign_historical_ctr_log'].mean().mean(),\n",
    "        \"campaign_success_percentile\": 0.5,\n",
    "        \"campaign_webpage_relative\": 1.0,\n",
    "        \"campaign_hour_relative\": 1.0,\n",
    "        \n",
    "        # campaign-specific metrics\n",
    "        \"campaign_ctrs\": df.groupby('campaign_id')['campaign_historical_ctr_log'].mean().to_dict(),\n",
    "        \"campaign_hour_performance\": campaign_hour_dict\n",
    "    }\n",
    "    \n",
    "    feature_statistics = {\n",
    "        \"historical_user_ctr_percentiles\": {\n",
    "            \"p25\": df['historical_user_ctr'].quantile(0.25),\n",
    "            \"p75\": df['historical_user_ctr'].quantile(0.75)\n",
    "        },\n",
    "        \"session_count_log_percentiles\": {\n",
    "            \"p25\": df['session_count_log'].quantile(0.25),\n",
    "            \"p75\": df['session_count_log'].quantile(0.75)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"engagement_defaults\": engagement_defaults,\n",
    "        \"campaign_defaults\": campaign_defaults,\n",
    "        \"feature_statistics\": feature_statistics\n",
    "    }\n",
    "\n",
    "def create_feature_params(df_train, features_to_scale, selected_features, output_path='feature_params.json'):\n",
    "    \"\"\"\n",
    "    Creates complete feature parameters and saves to JSON.\n",
    "    \"\"\"\n",
    "    # calculate empirical defaults\n",
    "    empirical_defaults = calculate_empirical_defaults(df_train)\n",
    "    \n",
    "    # combine all parameters\n",
    "    feature_params = {\n",
    "        \"features_to_scale\": features_to_scale,\n",
    "        \"selected_features\": selected_features,\n",
    "        \"engagement_defaults\": empirical_defaults[\"engagement_defaults\"],\n",
    "        \"campaign_defaults\": empirical_defaults[\"campaign_defaults\"],\n",
    "        \"feature_statistics\": empirical_defaults[\"feature_statistics\"],\n",
    "        \"categorical_features\": categorical_features,\n",
    "        \"keep_as_is\": keep_as_is\n",
    "    }\n",
    "    \n",
    "    save_feature_params(feature_params, output_path)\n",
    "    \n",
    "    return feature_params\n",
    "\n",
    "def save_feature_params(params, output_path):\n",
    "    \"\"\"Save feature parameters to JSON file with proper rounding.\"\"\"\n",
    "    def round_nested_dict(d, decimals=6):\n",
    "        if isinstance(d, dict):\n",
    "            return {k: round_nested_dict(v, decimals) for k, v in d.items()}\n",
    "        elif isinstance(d, float):\n",
    "            return round(d, decimals)\n",
    "        return d\n",
    "    \n",
    "    rounded_params = round_nested_dict(params)\n",
    "    \n",
    "    # save to JSON with proper formatting\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(rounded_params, f, indent=4)\n",
    "\n",
    "def load_feature_params(params_path):\n",
    "    \"\"\"Load feature parameters from JSON file.\"\"\"\n",
    "    with open(params_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "\n",
    "# create and save feature parameters\n",
    "feature_params = create_feature_params(\n",
    "    df_train=df_engineered,\n",
    "    features_to_scale=features_to_scale,\n",
    "    selected_features=final_selected_features,\n",
    "    output_path='feature_params.json'\n",
    ")\n",
    "\n",
    "# load parameters for test set feature creation\n",
    "loaded_params = load_feature_params('feature_params.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Probability distribution in test:\n",
      "Min: 0.2571\n",
      "Max: 0.4647\n",
      "Mean: 0.3632\n",
      "Std: 0.0529\n",
      "\n",
      "Using threshold 0.4288 to target 6.71% positive rate\n",
      "Achieved positive rate: 6.09%\n"
     ]
    }
   ],
   "source": [
    "def clean_test_data(df):\n",
    "    # remove entirely empty rows and fully duplicate rows\n",
    "    df = df.dropna(how=\"all\").drop_duplicates()\n",
    "    \n",
    "    # ensure the DateTime column is in datetime format\n",
    "    df[\"DateTime\"] = pd.to_datetime(df[\"DateTime\"])\n",
    "\n",
    "    # ensure values that are supposed to be ints are indeed so (and not unnecessarily floats)\n",
    "    int_columns = ['campaign_id', 'webpage_id', 'product_category_1',\n",
    "                   'age_level', 'user_depth', 'city_development_index']\n",
    "    for col in int_columns:\n",
    "        df[col] = df[col].apply(lambda x: int(x) if pd.notnull(x) else x)\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def impute_test_data(df, imputation_params):\n",
    "    \"\"\"\n",
    "    Applies imputation pipeline to test data using saved parameters.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "   \n",
    "    # hndle demographics using saved defaults\n",
    "    demographics = ['gender', 'age_level', 'user_depth', 'user_group_id']\n",
    "    for demo in demographics:\n",
    "        df[demo] = df[demo].fillna(imputation_params['demographics_defaults'][demo])\n",
    "    \n",
    "    # hndle product_category_2\n",
    "    df['product_category_2_missing'] = df['product_category_2'].isna().astype(int)\n",
    "    \n",
    "    # handle city_development_index\n",
    "    df['city_development_missing'] = df['city_development_index'].isna().astype(int)\n",
    "    df['city_development_index'] = df['city_development_index'].fillna(\n",
    "        imputation_params['city_development_mode']\n",
    "    )\n",
    "    \n",
    "    # handle low missing features if any\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().mean() < 0.01:  # threshold for low missing\n",
    "            if pd.api.types.is_object_dtype(df[col]) or df[col].nunique() <= 10:\n",
    "                # categorical variables\n",
    "                df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            else:\n",
    "                # numerical variables\n",
    "                df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def create_temporal_features(df):\n",
    "    \"\"\"Create temporal features while preserving order\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # time features\n",
    "    df['hour'] = df['DateTime'].dt.hour\n",
    "    df['day_of_week'] = df['DateTime'].dt.dayofweek\n",
    "    \n",
    "    # binary features\n",
    "    df['is_business_hours'] = df['hour'].between(9, 17).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_early_morning'] = df['hour'].between(2, 5).astype(int)\n",
    "    \n",
    "    # holiday features\n",
    "    us_holidays = holidays.US()\n",
    "    df['date'] = df['DateTime'].dt.date\n",
    "    df['is_holiday'] = df['date'].apply(\n",
    "        lambda x: bool(us_holidays.get(x)) if pd.notna(x) else 0\n",
    "    ).astype(int)\n",
    "    \n",
    "    df['is_near_holiday'] = df.apply(\n",
    "        lambda row: any(\n",
    "            abs((holiday - row['date']).days) <= 2\n",
    "            for holiday in us_holidays.keys()\n",
    "            if holiday <= row['date']\n",
    "        ),\n",
    "        axis=1\n",
    "    ).astype(int)\n",
    "    \n",
    "    # time of day\n",
    "    df['time_of_day'] = pd.cut(\n",
    "        df['hour'],\n",
    "        bins=[-np.inf, 6, 12, 18, np.inf],\n",
    "        labels=['night', 'morning', 'afternoon', 'evening']\n",
    "    )\n",
    "    \n",
    "    # hour bins\n",
    "    df['hour_bin'] = pd.cut(\n",
    "        df['hour'],\n",
    "        bins=[0, 4, 8, 12, 16, 20, 24],\n",
    "        labels=['dawn', 'early_morning', 'morning', 'afternoon', 'evening', 'night'],\n",
    "        include_lowest=True,\n",
    "        right=False\n",
    "    )\n",
    "    \n",
    "    df = df.drop('date', axis=1)\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def create_test_engagement_features(df, engagement_defaults):\n",
    "    \"\"\"Create engagement features using default values\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # apply default values from our saved parameters\n",
    "    for feature, default_value in engagement_defaults.items():\n",
    "        df[feature] = default_value\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def create_test_campaign_features(df, campaign_defaults):\n",
    "    \"\"\"Create campaign features using saved campaign-specific values when available\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # use campaign-specific CTR if available, otherwise use global default\n",
    "    df['campaign_historical_ctr_log'] = df['campaign_id'].astype(str).map(\n",
    "        campaign_defaults['campaign_ctrs']\n",
    "    ).fillna(campaign_defaults['campaign_historical_ctr_log'])\n",
    "    \n",
    "    # use campaign-hour specific performance if available\n",
    "    df['campaign_hour_key'] = df.apply(\n",
    "        lambda x: f\"{x['campaign_id']}_{x['hour']}\", axis=1\n",
    "    )\n",
    "    df['campaign_hour_relative'] = df['campaign_hour_key'].map(\n",
    "        campaign_defaults['campaign_hour_performance']\n",
    "    ).fillna(campaign_defaults['campaign_hour_relative'])\n",
    "    \n",
    "    # other campaign features\n",
    "    df['campaign_success_percentile'] = campaign_defaults['campaign_success_percentile']\n",
    "    df['campaign_webpage_relative'] = campaign_defaults['campaign_webpage_relative']\n",
    "    \n",
    "    df = df.drop('campaign_hour_key', axis=1)\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def create_interaction_features(df):\n",
    "    \"\"\"Create interaction features while preserving order\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # time-based interactions\n",
    "    df['campaign_hour_bin'] = df['campaign_id'].astype(str) + '_' + df['hour_bin'].astype(str)\n",
    "    df['campaign_early_morning'] = df['campaign_id'] * df['is_early_morning']\n",
    "    \n",
    "    # user depth interactions\n",
    "    df['user_depth_time'] = df['user_depth'].astype(str) + '_' + df['time_of_day'].astype(str)\n",
    "    df['age_weekend'] = df['age_level'] * df['is_weekend']\n",
    "    df['user_depth_age'] = df['user_depth'] * df['age_level']\n",
    "    \n",
    "    # session-time interaction\n",
    "    df['session_count_bin'] = pd.cut(\n",
    "        df['session_count_log'],\n",
    "        bins=5,\n",
    "        labels=['VL', 'L', 'M', 'H', 'VH']\n",
    "    )\n",
    "    df['user_sessions_time'] = df['session_count_bin'].astype(str) + '_' + df['time_of_day'].astype(str)\n",
    "    \n",
    "    # demographic interactions\n",
    "    df['gender_age'] = df['gender'].astype(str) + '_' + df['age_level'].astype(str)\n",
    "    \n",
    "    # geographic interactions\n",
    "    df['city_business'] = df['city_development_index'] * df['is_business_hours']\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\")\n",
    "\n",
    "\n",
    "def prepare_test_features_for_modeling(df, categorical_features, features_to_scale, keep_as_is):\n",
    "    \"\"\"\n",
    "    Prepares features specifically for test data modeling, ensuring essential features are preserved.\n",
    "    Similar to prepare_features_for_modeling but modified for test data needs.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # convert boolean columns to integers if any exist\n",
    "    bool_cols = df.select_dtypes(include='bool').columns\n",
    "    df[bool_cols] = df[bool_cols].astype(int)\n",
    "    \n",
    "    # handle ordinal features explicitly\n",
    "    ordinal_features = {\n",
    "        'age_level': range(7),\n",
    "        'user_depth': range(1, 4),\n",
    "        'city_development_index': range(1, 5)\n",
    "    }\n",
    "    \n",
    "    ordinal_feature_names = []\n",
    "    for col, categories in ordinal_features.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.Categorical(df[col], categories=categories, ordered=True).codes\n",
    "            ordinal_feature_names.append(col)\n",
    "\n",
    "    # one-hot encoding for categorical features\n",
    "    # remove from the categorical features list features that are also in \"keep_as_is\"\n",
    "    categorical_features = [col for col in categorical_features if col not in keep_as_is]\n",
    "    encoded_feature_names = []\n",
    "    for col in categorical_features:\n",
    "        if col in df.columns:\n",
    "            dummies = pd.get_dummies(df[col], prefix=col, drop_first=True, dtype=float)\n",
    "            df = pd.concat([df, dummies], axis=1)\n",
    "            df = df.drop(col, axis=1)\n",
    "            encoded_feature_names.extend(dummies.columns.tolist())\n",
    "    \n",
    "    # update categorical_features to include both ordinal and one-hot encoded features\n",
    "    categorical_features = ordinal_feature_names + encoded_feature_names\n",
    "\n",
    "    # remove only specific non-modeling features, preserving essential ones\n",
    "    features_to_drop = ['session_id', 'user_id', 'DateTime']\n",
    "    df = df.drop([col for col in features_to_drop if col in df.columns], axis=1)\n",
    "\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    return df.sort_values(\"_order_tracking\"), categorical_features\n",
    "\n",
    "\n",
    "def create_test_features(df, feature_params):\n",
    "    \"\"\"Create all features for test data while preserving order\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ensure DateTime is in datetime format\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'])\n",
    "    \n",
    "    # create features in sequence while maintaining order\n",
    "    df = create_temporal_features(df)\n",
    "    df = create_test_engagement_features(df, feature_params['engagement_defaults'])\n",
    "    df = create_test_campaign_features(df, feature_params['campaign_defaults'])\n",
    "    df = create_interaction_features(df)\n",
    "    \n",
    "    # get transformed dataframe\n",
    "    df_transformed, _ = prepare_test_features_for_modeling(\n",
    "        df,\n",
    "        categorical_features=feature_params['categorical_features'],\n",
    "        features_to_scale=feature_params['features_to_scale'],\n",
    "        keep_as_is=feature_params['keep_as_is'] + ['_order_tracking']\n",
    "    )\n",
    "    \n",
    "    # ensure all required dummy variables exist\n",
    "    for feature in feature_params['selected_features']:\n",
    "        if feature not in df_transformed.columns:\n",
    "            # if it's a dummy variable that wasn't created because the category wasn't present\n",
    "            if ('product_' in feature or 'gender_' in feature or \n",
    "                'time_of_day_' in feature or 'hour_bin_' in feature or\n",
    "                'session_count_bin_' in feature or 'user_depth_time_' in feature or\n",
    "                'user_sessions_time_' in feature or 'gender_age_' in feature):\n",
    "                df_transformed[feature] = 0\n",
    "    \n",
    "    # verify all required features are present\n",
    "    missing_features = set(feature_params['selected_features']) - set(df_transformed.columns)\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing required features: {missing_features}\")\n",
    "    \n",
    "    features_to_return = feature_params['selected_features'] + ['_order_tracking']\n",
    "    # ensure the df is sorted by \"_order_tracking\" when returned\n",
    "    ret = df_transformed[features_to_return].sort_values(\"_order_tracking\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def predict_with_model(X_test, model, target_positive_rate=0.067124):\n",
    "    \"\"\"\n",
    "    Generate predictions using saved model, targeting training CTR\n",
    "    \n",
    "    Parameters:\n",
    "        X_test: Features for prediction\n",
    "        model: Trained XGBoost model\n",
    "        target_positive_rate: Target CTR from training (default=6.71%)\n",
    "    \"\"\"\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    probabilities = model.predict(dtest)\n",
    "    \n",
    "    # find threshold that gives similar positive rate to training\n",
    "    threshold = np.percentile(probabilities, 100 - (target_positive_rate * 100))\n",
    "    \n",
    "    print(f\"\\nProbability distribution in test:\")\n",
    "    print(f\"Min: {probabilities.min():.4f}\")\n",
    "    print(f\"Max: {probabilities.max():.4f}\")\n",
    "    print(f\"Mean: {probabilities.mean():.4f}\")\n",
    "    print(f\"Std: {probabilities.std():.4f}\")\n",
    "    print(f\"\\nUsing threshold {threshold:.4f} to target {target_positive_rate:.2%} positive rate\")\n",
    "    \n",
    "    binary_predictions = (probabilities > threshold).astype(int)\n",
    "    actual_positive_rate = binary_predictions.mean()\n",
    "    print(f\"Achieved positive rate: {actual_positive_rate:.2%}\")\n",
    "    \n",
    "    return binary_predictions\n",
    "\n",
    "\n",
    "def process_test_data(test_filepath, imputation_params_path, feature_params_path, \n",
    "                     scaler_path, model_path):\n",
    "    \"\"\"Process test data while maintaining original order\"\"\"\n",
    "    # load and preserve order\n",
    "    test_df = pd.read_csv(test_filepath)\n",
    "    test_df['_order_tracking'] = np.arange(len(test_df))\n",
    "    \n",
    "    # load parameters\n",
    "    with open(imputation_params_path, 'r') as f:\n",
    "        imputation_params = json.load(f)\n",
    "    with open(feature_params_path, 'r') as f:\n",
    "        feature_params = json.load(f)\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "    # add the \"_order_tracking\" feature to the feature_params \"keep_as_is\"\n",
    "    feature_params['keep_as_is'].append('_order_tracking')\n",
    "\n",
    "    # basic cleaning\n",
    "    test_df = clean_test_data(test_df)\n",
    "    \n",
    "    # process features while maintaining order\n",
    "    test_df = impute_test_data(test_df, imputation_params)\n",
    "    test_df = create_test_features(test_df, feature_params)\n",
    "    \n",
    "    # scale features (which does not affect order of rows as it's column-wise)\n",
    "    X_test = test_df[feature_params['selected_features']]\n",
    "    X_test[feature_params['features_to_scale']] = scaler.transform(\n",
    "        X_test[feature_params['features_to_scale']]\n",
    "    )\n",
    "    \n",
    "    # predict :-)\n",
    "    predictions = predict_with_model(X_test, model)\n",
    "    \n",
    "    # write predictions to CSV\n",
    "    output_df = pd.DataFrame({'prediction': predictions})\n",
    "    output_df.to_csv('predictions.csv', index=False)\n",
    "    \n",
    "    return \n",
    "\n",
    "\n",
    "# generate predictions for test data\n",
    "predictions = process_test_data(\n",
    "    test_filepath='X_test_1st.csv',\n",
    "    imputation_params_path='imputation_params.json',\n",
    "    feature_params_path='feature_params.json',\n",
    "    scaler_path='feature_scaler.joblib',\n",
    "    model_path='final_model.joblib'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
